<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sargent Reading Group Notes</title>
    <link>http://srg.spencerlyon.com/index.xml</link>
    <description>Recent content on Sargent Reading Group Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>spencer.lyon@stern.nyu.edu (Spencer Lyon)</managingEditor>
    <webMaster>spencer.lyon@stern.nyu.edu (Spencer Lyon)</webMaster>
    <copyright>(c) 2015 Spencer Lyon.</copyright>
    <lastBuildDate>Tue, 31 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://srg.spencerlyon.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Y Yedid-Levi, S Haller, and D Fitzgerald (2017) (How Exporters Grow)</title>
      <link>http://srg.spencerlyon.com/2017/01/31/y-yedid-levi-s-haller-and-d-fitzgerald-2017-how-exporters-grow/</link>
      <pubDate>Tue, 31 Jan 2017 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2017/01/31/y-yedid-levi-s-haller-and-d-fitzgerald-2017-how-exporters-grow/</guid>
      <description>&lt;p&gt;The authors of this paper use confidential Irish data to document 4 novel facts about the lifecycle of exporting firms and then combine two existing modeling pieces to builds a partial equilibrium model that can reconcile the reported facts.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The authors use two confidential micro data sets from Ireland:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The Irish census of industrial production&lt;/li&gt;
&lt;li&gt;Irish custom records&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;They are able to link the datasets to build a panel dataset at the firm-product-destination market level.&lt;/p&gt;
&lt;h2 id=&#34;empirics&#34;&gt;Empirics&lt;/h2&gt;
&lt;p&gt;The main empirical exercise is to determine how one of log revenue, log quantity, or log price varies with both the firm-product duration in a particular market and the length of a firm-product-market export spell. The export spell is defined as the number of consecutive periods a firm exports a particular product to a particular market. Note that in the regressions this is a constant number for the entire spell, while the export tenure rises from 1 to the duration of the spell.&lt;/p&gt;
&lt;p&gt;The authors also control for destination market fixed effects and firm-product-year fixed effects.&lt;/p&gt;
&lt;p&gt;There are 4 key results from the estimation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Quantities grow dramatically in the first five years of successful export spells, defined as spells that last at least seven years. This growth is statistically significant up to a horizon of four years and is not driven purely by part-year effects in the first year (i.e. there is economically and statistically significant growth between years 2 and 4).&lt;/li&gt;
&lt;li&gt;Within successful export spells, there are no statistically or economically significant dynamics in prices.&lt;/li&gt;
&lt;li&gt;Higher initial quantities predict longer export spells: for spells lasting between one and four years, all pairwise comparisons of initial quantities are statistically different.&lt;/li&gt;
&lt;li&gt;Initial prices do not predict export spell length.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors do a number of robustness checks and report that the results are qualitatively unchanged when the data is cut differently or other controls are used.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;I now turn to the model. The use of the model is not as interesting or enlightening as the components themselves, so I will focus my discussion on why they made the assumptions they did.&lt;/p&gt;
&lt;p&gt;The authors make a quick note that common supply-side tricks for generating revenue and size dynamics (productivity shocks, capital adjustment costs, capacity or financial constraints, market-specific cost shocks, etc.) have a difficult time generating the observed dynamics in quantity without introducing dynamics in prices. For this reason they choose to focus on demand-side features that can generate dynamics.&lt;/p&gt;
&lt;p&gt;They choose two of the more common demand side bells and whistles to include in their model:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Learning about unobserved idiosyncratic shocks.&lt;/li&gt;
&lt;li&gt;Consumer capital: firms build up a consumer base that deprecates over time and add consumers by direct investment in marketing or other costly acquisition methods.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;After estimating the model with simulated method of moments (targeting moments about revenues and quantities over export spells), the authors show that the model can match all 4 of the key facts.&lt;/p&gt;
&lt;p&gt;They also show that both learning and consumer capital are necessary in their framework. To do this they remove one at a time, re-estimate the model, and show that the model generates price dynamics.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lustig, Zhang, and Hartman-Glaser (2016) (Capital Share Dynamics When Firms Insure Managers)</title>
      <link>http://srg.spencerlyon.com/2016/12/13/lustig-zhang-and-hartman-glaser-2016-capital-share-dynamics-when-firms-insure-managers/</link>
      <pubDate>Tue, 13 Dec 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/12/13/lustig-zhang-and-hartman-glaser-2016-capital-share-dynamics-when-firms-insure-managers/</guid>
      <description>&lt;p&gt;This paper uses Compustat data to document that the aggregate capital share of income and average capital share of income have diverged over the last 20 years. Specifically, the average capital share has fallen while the aggregate share has risen.&lt;/p&gt;
&lt;p&gt;They then write down a model that uses changes in firm level volatility of productivity to generate similar dynamics in general equilibrium.&lt;/p&gt;
&lt;p&gt;The model is interesting in its own right. It features a 2 sided limited commitment contracting problem. The agent has all bargaining power ex ante, but after the principal faces idiosyncratic shocks he ends up with capturing almost all aggregate rents ex post.&lt;/p&gt;
&lt;p&gt;This is related to a new literature on carefully modeling large firms in order to understand aggregates.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;The model is set in continuous time.&lt;/p&gt;
&lt;p&gt;Unit measure of ex ante identical firms. Each firm has an idiosyncratic time-varying productivity. Firms are owned by risk neutral investors (or shareholders) and operated by skilled managers. Firms rent physical capital and employ unskilled labor to produce using a decreasing returns to scale production technology.&lt;/p&gt;
&lt;p&gt;Firm productivity evolves as a geometric brownian motion that is subject to a negative poisson shock. If productivity reaches a strictly positive minimum value (determined in equilibrium), or if poisson process jumps, the firm exits immediately. There is a competitive fringe of potential entrants. When an investor creates a new firm they pay a fixed cost &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;P&lt;/em&gt;&lt;/span&gt;. Each period firms enter until the expected value of entering is equal to &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;P&lt;/em&gt;&lt;/span&gt; (free entry). After entering the initial productivity level is drawn from a Pareto distribution.&lt;/p&gt;
&lt;p&gt;When a firm is formed, but before initial productivity is realized, the investor is matched with a risk adverse manager and the investor offers a long term contract. The contract is a sequence of payments to be made to the manager at each instant. The manager has convex preferences over this sequence of payments and discounts the future at a constant rate. The manager can choose to accept or reject the proposed contract. Upon rejection the manager is immediately matched with a new potential entrant.&lt;/p&gt;
&lt;p&gt;Both parties are free to terminate the contract at any time. If the manager walks away from the firm, he will receive some outside option that is determined in equilibrium. The investor will choose to continue operations as long there is a positive net present value. The investor faces an optimal stopping time problem that characterizes when the firm is abandoned. The abandonment problem is to maximize the present discounted value of profits less managerial payments from the current period until the random stopping time, subject to the constraint that the manager doesn&#39;t walk away. The constraint sets the NPV of managerial payments plus post-stopping value equal to the outside option. This constraint is always binding, so WLOG the authors restrict attention to contracts that gives the manager a fixed payment until the firm exits.&lt;/p&gt;
&lt;h2 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h2&gt;
&lt;p&gt;The authors use this model to describe how to match the facts about the capital share of income. The main mechanism is that an increase in the volatility parameter in the productivity process will do two main things:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;raise the real option value of waiting to abandon for firms with low productivity. This results in a decrease in the lower productivity threshold at which firms exit&lt;/li&gt;
&lt;li&gt;Increase the number of firms with very high productivity.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In other words, higher volatility in innovations to productivity creates fat tails in the distribution of productivity across firms.&lt;/p&gt;
&lt;p&gt;The average capital share of income is computed as the integral of profits less managerial payments divided by profits over all firms.&lt;/p&gt;
&lt;p&gt;The aggregate capital share of income is computed as the integral of profits less managerial payments over all firms divided by the integral of profits over all firms.&lt;/p&gt;
&lt;p&gt;The fat right tail will put more mass on the extremely productive firms who are earning profits far above the managerial payments. This drives up the aggregate capital share of income.&lt;/p&gt;
&lt;p&gt;On the other hand, the additional mass of low productivity firms will bring the average capital share of income down.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timoshenko (2015) (Learning, prices, and firm dynamics)</title>
      <link>http://srg.spencerlyon.com/2016/12/06/timoshenko-2015-learning-prices-and-firm-dynamics/</link>
      <pubDate>Tue, 06 Dec 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/12/06/timoshenko-2015-learning-prices-and-firm-dynamics/</guid>
      <description>&lt;p&gt;The authors of this paper use firm level data on Portuguese manufacturing firms and documents two new novel facts about the joint evolution of firm performance and prices:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Within product categories, firms with longer tenure in export markets tend to export larger quantities at similar prices&lt;/li&gt;
&lt;li&gt;Older or more experienced exporters tend to import more expensive inputs&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The authors then write down a model that can match these facts&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;Time is discrete, there are N countries. Each country is populated by a unit mass of infinitely lived consumers. There re two sectors in each economy: a final goods sector and an intermediate inputs sector. The final goods sector is populated by a mass of monopolistically competitive firms that each supply a differentiated variety of varying quality. The intermediate inputs sector is perfectly competitive and operates with a CRS production technology.&lt;/p&gt;
&lt;p&gt;Consumers have log preferences over a CES aggregation of final goods. Within the CES aggregator final good has a multiplicative term representing the firm chosen quality of the good as well as a multiplicative preference shock term. This is idiosyncratic across firms.&lt;/p&gt;
&lt;p&gt;TODO: DESCRIBE WHAT THE STRUCTURE IS.&lt;/p&gt;
&lt;h4 id=&#34;intermediate-sector&#34;&gt;intermediate sector&lt;/h4&gt;
&lt;p&gt;Perfect competition leads to price being equal to marginal cost&lt;/p&gt;
&lt;h4 id=&#34;final-goods&#34;&gt;final goods&lt;/h4&gt;
&lt;p&gt;Static problem: choose good quality and quantity based on expected profits in each economy.&lt;/p&gt;
&lt;p&gt;Entry problem: given result of static problem, firms choose each period which markets to enter. If they enter they pay a fixed cost per market.&lt;/p&gt;
&lt;p&gt;For each market the firm serves, they observe the equilibrium price that cleared the market for their good. They invert this price to uncover what the demand shock was in that period. They use this demand shock as a noisy signal though which they update their beliefs about the distribution of demand shocks they face.&lt;/p&gt;
&lt;h4 id=&#34;eqm&#34;&gt;eqm&lt;/h4&gt;
&lt;p&gt;They show that this model can generate dynamics that match the two empirical facts from before.&lt;/p&gt;
&lt;h2 id=&#34;counterfactual&#34;&gt;Counterfactual&lt;/h2&gt;
&lt;p&gt;They also do a very brief counterfactual exercise where they consider the welfare implications of imposing a minimum quality standard on all exported goods. They find that this limit decreases welfare. The mechanism is as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;quality standard raises productivity threshold for entry into export market&lt;/li&gt;
&lt;li&gt;The decline in number of exporters reduces competition in each market, raising prices&lt;/li&gt;
&lt;li&gt;Another effect is less foreign firms allows for more low efficiency domestic firms to enter&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There is also an impact on the intensive margin:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;firms close to the productivity threshold (small or young) will face a binding quality constraint&lt;/li&gt;
&lt;li&gt;in order to export these firms must comply and in order to do so they need to pick which quality intermediate inputs.&lt;/li&gt;
&lt;li&gt;higher quality inputs requires more labor&lt;/li&gt;
&lt;li&gt;This leads in a reallocation of resources among exporters toward young and small firms.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TODO FILL THIS IN!!!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hess:2000wh (Risk sharing by households within and across regions and industries. (Hess, Shin; 2000))</title>
      <link>http://srg.spencerlyon.com/2016/11/22/hess2000wh-risk-sharing-by-households-within-and-across-regions-and-industries.-hess-shin-2000/</link>
      <pubDate>Tue, 22 Nov 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/11/22/hess2000wh-risk-sharing-by-households-within-and-across-regions-and-industries.-hess-shin-2000/</guid>
      <description>&lt;p&gt;&lt;p&gt;The goal of this paper is to test the degree of risk sharing across regions and industries in the US.&lt;/p&gt;
&lt;p&gt;The rough outline is to write down a simple model they use to motivate an empirical specification, then test if that specification holds in the data.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;There are H households that each live in one of R regions and work within one of I industries. Each period, households receive an endowment of the consumption good and are able to trade in complete state contingent markets. Households have time-separable CRRA preferences that are subject to multiplicative preference shocks.&lt;/p&gt;
&lt;p&gt;The authors formulate the social planner&amp;rsquo;s problem for the entire economy. The first order condition for each household equates the Lagrange multiplier on the resource constraint to the Pareto weight times marginal utility of consumption.&lt;/p&gt;
&lt;p&gt;They then take logs, sum across all households, and take first differences to obtain a key equation that says household log consumption growth is equal to aggregate log consumption growth plus the difference in deviations of individual preference shocks from the average preference shock times a scale factor.&lt;/p&gt;
&lt;p&gt;There are two key things to understand about this relationship:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There is perfect within period risk sharing across households as their consumption is independent of their endowment.&lt;/li&gt;
&lt;li&gt;There is also intertemporal consumption smoothing because the innovation to permanent income is the same across households because it is linked to aggregate consumption growth. When solving the planner&amp;rsquo;s problem it is difficult to write down the permanent income process, but that&amp;rsquo;s how it works.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;They also consider market structures where there is perfect risk sharing only within a region or within an industry, but not across these groups. They solve for allocations in the within-group risk sharing economy using a social planner problem, just like before. The only difference is that the new planners only consider the agents in one group.&lt;/p&gt;
&lt;p&gt;The optimality conditions are identical and that important equation relates individual consumption growth to group-wise aggregate consumption growth and preference shocks is the same. If you then aggregate these within-group conditions across the groups you arrive at the same condition we got at the end of the economy-wide planner&amp;rsquo;s problem, with the addition of one extra term. This term is a risk adjusted difference in group and economy Lagrange multipliers.&lt;/p&gt;
&lt;p&gt;In order to derive the equation they use as the main specification for the empirical exercises, they need to remove these unobservable Lagrange multipliers. To do this they return one more time to optimality conditions for the different planners and manipulate them to write the difference in Lagrange multipliers as the difference in group and economy level consumption growth.&lt;/p&gt;
&lt;h2 id=&#34;empirics&#34;&gt;Empirics&lt;/h2&gt;
&lt;p&gt;The final equation they end up taking to the data is that individual log consumption growth is equal to the sum of aggregate log consumption growth, the difference between regional and aggregate consumption growth, the difference between industry and aggregate consumption growth, controls for changes in preferences, and individual income growth.&lt;/p&gt;
&lt;p&gt;They use data from the PSID to run regressions on this model. The theory suggests that if there is complete risk sharing across regions and industries, that the coefficients on those terms should be zero. They find that these coefficients are significantly different from zero, suggesting that a large fraction of wealth is incompletely shared across regions and industries.&lt;/p&gt;
&lt;!-- The LHS of the economy wide and group specific equations is individual
consumption growth, which allows them to write the difference between economy
level and within group aggregate consumption as a statement about individual
preference shocks. Applying a law of large numbers will cause these individual
deviations from aggregate preference shocks to aggregate out and the
implication is that economy wide an within group consumption growth should be
the same. --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adam:2011ix (Adam, K., &amp; Marcet, A. (2011). Internal rationality, imperfect market knowledge and asset prices)</title>
      <link>http://srg.spencerlyon.com/2016/11/01/adam2011ix-adam-k.--marcet-a.-2011.-internal-rationality-imperfect-market-knowledge-and-asset-prices/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/11/01/adam2011ix-adam-k.--marcet-a.-2011.-internal-rationality-imperfect-market-knowledge-and-asset-prices/</guid>
      <description>&lt;p&gt;&lt;p&gt;The basic idea in this paper is to separate the standard rationality requirements embedded in the rational expectations hypothesis into internal and external components. &lt;em&gt;Internal rationality&lt;/em&gt; means that the agents make fully optimal decisions given some well-defined subjective beliefs about payoff relevant variables. &lt;em&gt;External rationality&lt;/em&gt; requires that the probability distribution generated by agent subjective beliefs matches the true distribution of underlying variables.&lt;/p&gt;
&lt;p&gt;This paper will maintain the assumption of internal rationality, but not impose external rationality. To demonstrate the implications of this change, the authors use a simple Lucas asset pricing model to describe how the evolution of prices is different with and without extra rationality.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;Time is discrete. There are I infinitely-lived investor types, each with unit mass. Each agent is endowed with an equal share to an infinitely lived Lucas tree that stochastically yields consumable dividends Dt each period.&lt;/p&gt;
&lt;p&gt;Agents have risk-neutral, time separable preferences over streams of consumption. The discount factor and probability measure used by agents is type-specific. Each agent chooses a sequence of consumption and asset holdings to maximize the expected discounted value of consumption subject to budget constraint and that asset holdings are between 0 and some (large) upper bound each period. The budget constraint requires that consumption plus the cost of asset purchases is less than the sum of asset sales; dividend receipts; and an fixed, exogenous endowment of the consumption good. Each period the price of the asset is Pt.&lt;/p&gt;
&lt;p&gt;The non-standard part of the setup is that agents form beliefs over both realizations of asset prices and dividends. Under the REH, we typically assume that beliefs are over only dividend realizations and that agents know a mapping between dividends and prices when computing expectations.&lt;/p&gt;
&lt;p&gt;In this model, internal rationality is that each agent chooses consumption and asset holdings to maximize expected discounted utility subject to the constraints, taking their type&amp;rsquo;s probability measure as given.&lt;/p&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;Agent&amp;rsquo;s optimality conditions are standard. An interior solution equates the current price with the discounted expected price plus dividends tomorrow. Without external rationality, agents have joint beliefs over prices and dividends, so they use this first order condition to derive the equilibrium price. Because discounting and expectations are formed type be type, the equilibrium price will be the maximum of this discounted expectation over all types.&lt;/p&gt;
&lt;p&gt;With external rationality (i.e. under standard rational expectations conditions) agents only have beliefs over the dividend process. They would use this first order condition together with the law of iterated expectations write the price today as the present discounted value of all future dividend payments.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a step back and think about this&amp;hellip;&lt;/p&gt;
&lt;p&gt;Note that we can consider external rationality as a special case of the model without external rationality. Specifically with external rationality agents are given a probability measure over dividends &lt;em&gt;and&lt;/em&gt;, implicitly, a mapping from histories of dividend realizations to prices. Knowledge of this function is &lt;em&gt;not&lt;/em&gt; an outcome of agent maximization (i.e. internal rationality), but rather the impact of a set of assumptions the modeler makes about what agents know about how the market operates. Given that economists haven&amp;rsquo;t found a mapping from dividend streams to prices, it seems reasonable to assume that agents don&amp;rsquo;t have this mapping either.&lt;/p&gt;
&lt;h3 id=&#34;internal-only-to-reh&#34;&gt;Internal only to REH&lt;/h3&gt;
&lt;p&gt;We now consider which assumptions are needed to go from the internal rationality only model to the model with both internal and external rationality. That is, we consider assumptions that allow our agents to write the current price as a expected discounted present value of future dividend payments.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It is common knowledge that a single agent &amp;quot;sets the price&amp;quot; each period. I call this agent the marginal agent. This allows each agent to use &lt;em&gt;their&lt;/em&gt; own beliefs about who is marginal each period to write today&amp;rsquo;s price as the present discounted value of dividends.&lt;/li&gt;
&lt;li&gt;It is common knowledge that the last term in the infinite sum is zero, when the marginal agent&amp;rsquo;s beliefs and discount factor are applied each period. This gives agents information about the market in that all agents expect all future marginal agents to expect (and so on&amp;hellip;) that prices grow slower than the marginal discount factors. This is a no rational bubbles condition.&lt;/li&gt;
&lt;li&gt;All agents know which agent is marginal each period &lt;em&gt;and&lt;/em&gt; what the marginal agent&amp;rsquo;s discount factor and probability measure are. This allows all agents to write down the same infinite sum, that coincides with the equilibrium part.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adam:2016hd (Adam, K., &amp; Marcet, A. &amp; Nicolini J. (2016). Stock Market Volatility and Learning.)</title>
      <link>http://srg.spencerlyon.com/2016/11/01/adam2016hd-adam-k.--marcet-a.--nicolini-j.-2016.-stock-market-volatility-and-learning./</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/11/01/adam2016hd-adam-k.--marcet-a.--nicolini-j.-2016.-stock-market-volatility-and-learning./</guid>
      <description>&lt;p&gt;Builds on the internal rationality framework from last week to build a model of asset pricing that can explain 5 facts that have puzzled the literature at one time or another.&lt;/p&gt;
&lt;h2 id=&#34;facts&#34;&gt;Facts&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Standard deviation of price dividend ratio is very high (about 1/2 the mean of the PD ratio)&lt;/li&gt;
&lt;li&gt;First-order quarterly autocorrelation of PD ratio is vary high&lt;/li&gt;
&lt;li&gt;Standard deviation of stock returns is almost 4 times as large as standard deviation of dividend growth&lt;/li&gt;
&lt;li&gt;PD ratio is good long run predictor of stock returns&lt;/li&gt;
&lt;li&gt;Equity premium puzzle: return on risky stocks is too high relative to bonds for standard models&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;There is a unit mass of infinitely lived investors, endowed with one unit of a stock that can be traded in a competitive market and that pays a per period dividend D in units of a perishable consumption good. The log growth rate of dividends follows an AR(1).&lt;/p&gt;
&lt;p&gt;Agents also receive a stochastic endowment of the consumption good. The feasibility constraint requires that consumption be equal to dividends plus the endowment. Following the consumption-based asset pricing literature they choose to model the consumption process directly instead of the income process. The log growth of consumption is also an AR(1). The innovations in consumption growth and dividend growth are correlated with coefficient equal to 0.2 (estimated in the data).&lt;/p&gt;
&lt;p&gt;In addition to trading shares of the dividend yielding risky asset, agents can also trade in a one period risk free bond.&lt;/p&gt;
&lt;p&gt;All agents discount the future at the same, constant rate.&lt;/p&gt;
&lt;p&gt;The objective of each consumer is to choose sequences of functions that map histories of observed prices, dividends, and endowments into consumption, stock holdings, and bond holdings to maximize the expected discounted utility of consumption; subject to a budget constraint that equates expenditure on consumption and stock and bond purchases to dividends, the endowment and bond returns.&lt;/p&gt;
&lt;p&gt;Each agent is allowed to have subjective beliefs over the joint evolution of prices, dividends, and endowments. Agents behave fully rationally, given these beliefs. The only difference between the setup here and the classic setup is that agents form beliefs over prices instead of being assumed to have a mapping between histories of dividends and endowments into current prices. These subjective beliefs will be updated over time and are crucial for the model to explain the facts in the data.&lt;/p&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;p&gt;The agents first order necessary conditions for stocks and bonds are standard.&lt;/p&gt;
&lt;p&gt;A key equilibrium result is the pricing function that relates current dividends and expected price and dividend growth into current prices.&lt;/p&gt;
&lt;p&gt;This function is that prices today are equal to risk adjusted expected dividend growth, divided by one minus risk adjusted expected price growth, times the discount factor, times dividends. It is crucial that todays price is increasing the divided flow, risk-adjusted expectations about dividend growth, and risk-adjusted expectations about price growth.&lt;/p&gt;
&lt;p&gt;Under the rational expectations hypothesis, the expectation of both of these growth rates is constant. This results in growth rate of prices to exactly equal the growth rate of dividends. It is for this reason that the standard rational expectations model fails to match the first 4 facts I gave before. The model also generates a low risk premium, so it misses all 5 facts.&lt;/p&gt;
&lt;p&gt;However, when beliefs are subjective the expected risk-adjusted growth rates are not constant over time. The authors argue that the important component of their model is fluctuations in the expectation of price growth. To test this, they assume that investors use the true probability measure for divided growth, but hold subjective beliefs over price growth. This is the same as assuming agents still cannot map perfectly from dividend and endowment realizations into prices.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The baseline parameterization of the proposed model is able to quantitatively match all the facts except the risk premium. In this model the CRRA parameter is set to 5. If they allow this to float to 80, they are also able to match the risk premium fact.&lt;/p&gt;
&lt;p&gt;The question is, how does it happen?&lt;/p&gt;
&lt;p&gt;The key mechanism is a feedback loop between expectations about price growth and the realization of price growth. Here&#39;s a rough sketch of how it works&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expected price growth exhibits both momentum and mean reversion to the rational expectants &amp;quot;fundamental level&amp;quot; of prices&lt;/li&gt;
&lt;li&gt;This momentum means that beliefs have a tendency to increase further following an initial increase whenever beliefs are at or below their fundamental value.&lt;/li&gt;
&lt;li&gt;Mean reversion means that beliefs can&#39;t stay away from the rational expectations level forever.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These facts together cause the price dividend ratio to fluctuate around its rational expectation value. This allows the model to explain the observed volatility and serial correlation of the PD ratio (facts 1 and 2). This behavior also causes stock returns to be more volatile than dividend growth; which explains fact 3. Finally, serial correlation and mean-reversion in the PD ratio give rise to excess return predictability -- fact 4 (NOTE: the notion of predictability is explained in another paper by Cochrane that I didn&#39;t study).&lt;/p&gt;
&lt;p&gt;The model still fails, however to generate a high enough risk premium. In order to match this fact, they authors have to adjust risk aversion. They do so by cranking up the CRRA parameter. I believe that using recursive preferences and adjusting the inter temporal elasticity of substitution is another way to generate excess returns without resorting to very high levels of risk aversion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Baley:2016wr (Firm uncertainty cycles and the propagation of nominal shocks)</title>
      <link>http://srg.spencerlyon.com/2016/10/25/baley2016wr-firm-uncertainty-cycles-and-the-propagation-of-nominal-shocks/</link>
      <pubDate>Tue, 25 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/25/baley2016wr-firm-uncertainty-cycles-and-the-propagation-of-nominal-shocks/</guid>
      <description>&lt;p&gt;&lt;p&gt;This paper develops a model with menu costs for adjusting prices and imperfect information about idiosyncratic productivity shocks. They conduct monetary policy experiments and conclude that the distribution of firm level uncertainty is important for the propagation of monetary shocks.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;In this model time is continuous. There is a representative consumer, a continuum of monopolistically competitive firms, and a monetary authority.&lt;/p&gt;
&lt;p&gt;In the baseline model, the monetary authority keeps the money supply fixed at its initial level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The representative consumer&lt;/strong&gt; has log preferences over consumption and money holdings, with linear disutility from labor. The household discounts the future at a constant rate r. Consumption is the CES aggregation of firm specific goods, each multiplied by a firm specific quality shock. The lifetime budget constraint of the consumer says that the value of consumption expenditures plus the cost of holding money, less labor earnings and firm profits is no greater than the initial money supply.&lt;/p&gt;
&lt;p&gt;There are a continuum of &lt;strong&gt;firms&lt;/strong&gt; who each product and sell their product in a monopolistically competitive market. Each firm has access to a linear technology that produces one unit of output for each unit of labor, divided by the stochastic quality of the good (meaning a higher quality good requires more labor to produce).&lt;/p&gt;
&lt;p&gt;This is the same quality that appears in the CES aggregator for consumption. The log of the quality shock follows a jump-diffusion process, without drift. This means that &lt;code&gt;da(z) = sigma1 dW + sigma2 u dQ&lt;/code&gt;, where W is a Weiner process and u Q is a poisson process with standard normal innovations. It is assumed that sigma1 is much smaller than sigma2, such that when the poisson process jumps there is a large shock to the quality of the firm&amp;rsquo;s product.&lt;/p&gt;
&lt;p&gt;Firms do not observe the quality directly and cannot learn about it using wage costs. The only information they receive about their quality is a noisy signal and the information about when the poisson process jumps. The evolution of the signal follows &lt;code&gt;ds = a dt + gamma dZ&lt;/code&gt;, where Z is another independent Weiner process.&lt;/p&gt;
&lt;p&gt;Firm prices are subject nominal rigidities in which firms can only change their price if they pay a fixed menu cost &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The flow profit to a firm is given by the demand for the good, multiplied by the price less the wage bill.&lt;/p&gt;
&lt;p&gt;Firms use the discount rate of the household (because households own firms) and seek to maximize the expected discounted profit streams by choosing a sequence of prices and stopping times for price adjustments. When firms adjust prices, it is optimal to set them to a constant markup over marginal costs. Each period the firm does not adjust its price, there is a gap between the optimal markup in a frictionless economy (i.e. without both price and information frictions) and the markup being used by the firm. The equilibrium flow of profits can be expressed as minus a constant times this markup gap.&lt;/p&gt;
&lt;p&gt;Because firms cannot directly observe the quality of their goods, they do not know the true value of the markup gap each period. Instead, they use the signals about the quality shock to form a signal extraction problem. A main contribution of the paper is the extension of the Kalman-Bucy filter to the environment where the hidden state follows a jump-diffusion process. One key output of the system of filtering equations is that innovations in the estimate of the markup gap are more volatile when there is high uncertainty about the estimate &amp;ndash; e.g. when the variance of the estimate is high. An implication of this is that when uncertainty is high, firms place higher weight on their signals than they do on the current estimate when updating beliefs. In this scenario, the learning rate is higher, but also noisier.&lt;/p&gt;
&lt;p&gt;The optimal stopping time for firms is characterized by an inaction region &amp;ndash; as long as the filtered estimate for the markup gap is within certain bounds, the firm does not update prices. Once the price touches one of the borders, it immediately adjusts prices to set the estimated markup gap to zero.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Here are some key results about the steady state equilibrium of this economy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The filtering equation for updating uncertainty (the variance of the state estimate) produces cycles of uncertainty for the firm. There is a deterministic component that causes uncertainty to fall to it&amp;rsquo;s minimum level (the volatility of the diffusion component of quality shocks) and a stochastic component that causes it to rise whenever Poisson process jumps. The time series for a simulated path has a saw-tooth pattern where sharp increases are followed by gradual declines in uncertainty.&lt;/li&gt;
&lt;li&gt;In times when uncertainty is high, the innovations to the filtered estimate of the markup gap are more volatile. This generates cycles in firm markup behavior where in times of high uncertainty firms choose to update prices more frequently. These cycles in uncertainty and pricing behavior are idiosyncratic because they are driven by idiosyncratic evolution of Brownian Motion and Poisson processes.&lt;/li&gt;
&lt;li&gt;The idiosyncratic cycles allow the authors to look at how the &lt;em&gt;distribution&lt;/em&gt; of uncertainty relates to the response to monetary shocks. To do this they perform an experiment where the monetary authority does a one-time un-anticipated increase in the money supply by a known amount. When the monetary shock occurs, estimates of the markup gap are updated immediately. But, a firm&amp;rsquo;s price will only get update when its estimate leaves its inaction region. As long as some firms&amp;rsquo; estimates remain in the inaction region, aggregate output will be different from its post-shock steady state value. As a summary statistic, they compute the total output effect, which is the integral over all time of the output gap relative to the steady state. They find that in the economy with information frictions, the output effect is larger up to 7 times larger and has a half life up to 5.3 times longer half life than in the perfect information economy. In this sense the authors claim that the distribution of firm level uncertainty is an important consideration when trying to evaluate the effects of monetary policy actions.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ikegami:2016ek (Poverty Traps and the Social Protection Paradox)</title>
      <link>http://srg.spencerlyon.com/2016/10/18/ikegami2016ek-poverty-traps-and-the-social-protection-paradox/</link>
      <pubDate>Tue, 18 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/18/ikegami2016ek-poverty-traps-and-the-social-protection-paradox/</guid>
      <description>&lt;p&gt;This paper presents and analyzes a stylized model of poverty traps in developing economies.&lt;/p&gt;
&lt;h2 id=&#34;baseline-model&#34;&gt;Baseline model&lt;/h2&gt;
&lt;p&gt;In the baseline model there are a finite number of agents.&lt;/p&gt;
&lt;p&gt;Agent&#39;s are characterized by a constant level of innate ability &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;α&lt;/em&gt;&lt;/span&gt; and an evolving stock of capital &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Each period, agents choose which of two DRS production technologies they wish to operate in that period. Switching technologies is costless. Both technologies have innate ability multiplied by physical capital raised to a power less than one. The more productive technology has a higher exponent, but requires payment of a fixed cost to operate.&lt;/p&gt;
&lt;p&gt;Agent&#39;s have CRRA preferences over consumption.&lt;/p&gt;
&lt;p&gt;Agents choose consumption and technology each period to maximize the expected discounted value of lifetime utility from consumption, subject to a few constraints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consumption can be no more than capital stock plus production output&lt;/li&gt;
&lt;li&gt;The capital stock is always non-negative&lt;/li&gt;
&lt;li&gt;Next period capital stock is the unconsumed portion of resources multiplied by an asset shock less depreciation. When this shock is less than unity, some capital is destroyed. The shocks are drawn iid from a known distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-solution&#34;&gt;Model Solution&lt;/h2&gt;
&lt;p&gt;The dynamics induced by agents&#39; optimal behavior can be understood by considering two interacting effects.&lt;/p&gt;
&lt;p&gt;First, the choice of production technology is governed by a cutoff rule for capital as a function of innate ability. If an agent&#39;s capital stock is above this threshold, they will attempt to accumulate capital so that they can employ the high productivity technology. Otherwise the agent will only pursue the low technology and accumulate the relatively smaller stock of capital needed to operate that technology optimally.&lt;/p&gt;
&lt;p&gt;Second, the state space can be partitioned into three regions along the innate ability dimension&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The unskilled worker region, where regardless of the amount of capital agents always find it optimal to use the low productivity technology. For each innate ability level, there is a unique optimal capital stock to target. Poverty is defined as having capital stock equal to this level.&lt;/li&gt;
&lt;li&gt;A high skill region where for all levels of capital agents prefer to operate the high productivity technology.&lt;/li&gt;
&lt;li&gt;An intermediate region where depending on the current capital stock and sequence of asset shocks, agents may choose to operate either technology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These regions define two forms of poverty trap:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Low skill agents who will always be in poverty&lt;/li&gt;
&lt;li&gt;Middle skill agents who are vulnerable to being pushed into poverty if they receive sufficiently unfavorable asset shocks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;policy&#34;&gt;Policy&lt;/h2&gt;
&lt;p&gt;The authors use this framework to analyze a few competing forms of government intervention. They analyze the effectiveness of each policy using a simulation experiment. In each experiment they randomly initialize 300 agents to be 25% in the low and high skill region and 50% in the intermediate region. The capital stock is initialized by independent draws from uniform distribution on [0, 10]. They they simulate the model forward under a given policy for 50 periods and track the distribution of agents.&lt;/p&gt;
&lt;p&gt;As a baseline, we first consider autarky, or no government transfer program. The results of the simulation are a clear increase in the poverty level relative to the initial conditions. At the start, about 60% of the population chose to operate the high productivity technology. At then end of the simulation this has dropped to 40%.&lt;/p&gt;
&lt;p&gt;The first policy considered is a progressive policy that targets the poorest agents in the population. All agents below the poverty level are given a transfer that brings them exactly to the poverty line. If there are insufficient government funds, each agent is given a share of total government resources proportional to their distance from the poverty line. Agents do not anticipate the transfers. The results of simulating in this environment are qualitatively identical to autarky.&lt;/p&gt;
&lt;p&gt;The second policy targets the middle skill agents near the cutoff rule for switching between production technologies. Specifically, if an agent starts the period in the high technology region and recipes a poor enough asset shock to move to the low region -- the government provides a transfer that brings the agent exactly back to the cutoff level. Again the transfer is unanticipated. The simulation results here are quite different. Aggregate output rises by 10% and poverty falls from 55% to 25% -- meaning 75% of agents operate the high productivity technology.&lt;/p&gt;
&lt;p&gt;The difference between these two policies brings up an ethical issue regarding which subset of agents government policies should target. The authors provide some discussion, but leave it as an open ended question.&lt;/p&gt;
&lt;p&gt;The final experiment also targets the middle skill agents near the cutoff, but this time the transfers are anticipated. The anticipation brings about two competing moral hazard forces:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The positive force is that when agents know they will receive a transfer if their investment is subject to poor shocks, they choose to invest more.&lt;/li&gt;
&lt;li&gt;The negative force is that agents would like to remain as close to the cutoff as possible so they can get the transfer more often.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results of the simulation under these conditions falls between the two previous examples. The authors don&#39;t report numbers, so I can&#39;t be precise. However, from the figures the main takeaway is that agents in the middle skill region who are close to the production cutoff are now much less likely to end up operating the low productivity technology in the long run.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaplan:2016fe (Non-durable Consumption and Housing Net Worth in the Great Recession: Evidence from Easily Accessible Data.)</title>
      <link>http://srg.spencerlyon.com/2016/10/11/kaplan2016fe-non-durable-consumption-and-housing-net-worth-in-the-great-recession-evidence-from-easily-accessible-data./</link>
      <pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/11/kaplan2016fe-non-durable-consumption-and-housing-net-worth-in-the-great-recession-evidence-from-easily-accessible-data./</guid>
      <description>&lt;p&gt;&lt;p&gt;In 2013; Mian, Rao, Sufi used proprietary data on the US housing market (obtained from Core Logic) and personal consumption expenditures (obtained from master card) from 2006-2009 to estimate that the elasticity of consumption expenditures to changes in the housing share of household net worth.&lt;/p&gt;
&lt;p&gt;This paper replicates the main results from Mian, Rao, and Sufi using data that is more easily accessed by economists in academia.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The authors proxy the housing data from Core Logic, using housing data from Zillow. The data is freely downloadable from the Zillow website.&lt;/p&gt;
&lt;p&gt;Instead of the mastercard data, the authors use data from the Kilts Neilsen scanner retail survey. This survey includes weekly price and quantity levels for sales at the bar code level for about 40,000 US stores from 2006-2009 (the panel is still ongoing and currently runs to 2014). KMV estimate that this subset is approximately 40% of aggregate US consumption on non-durables.&lt;/p&gt;
&lt;h2 id=&#34;replication&#34;&gt;Replication&lt;/h2&gt;
&lt;p&gt;Using the Core logic and mastercard data to, MRS report an elasticity of consumption expenditures to changes in the housing share of household net worth between 0.33 and 0.36 (depending on the controls in the regression and regression technique &amp;ndash; OLS vs IV2SLS).&lt;/p&gt;
&lt;p&gt;Using the Zillow and Kilts Neilsen data, KVM report an elasticity between 0.24 and 0.36.&lt;/p&gt;
&lt;p&gt;The similarity of these findings despite the very different data sets is encouraging.&lt;/p&gt;
&lt;h2 id=&#34;new-contributions&#34;&gt;New contributions&lt;/h2&gt;
&lt;p&gt;In addition to replicating the results from MRS, KMV have 3 main findings:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They show that the interaction between the fall in local house prices and the size of initial leverage is not statistically significant, after controlling for the direct impact of house price changes.&lt;/li&gt;
&lt;li&gt;They separate the price and quantity components in the fall in consumption expenditure during the great recession. They construct a proxy measure of the quantity of household expenditure by aggregating quantity sold from all stores at the product level, and then multiplying by an average price for the product. When this is used as the dependent variable in the regression, elasticities are approximately 20% lower.&lt;/li&gt;
&lt;li&gt;They use the Diary Survey of the Consumer Expenditure Survey to estimate the elasticity of total non-durable goods and survives to the counter part found in the Kilts Nielsen dataset. They obtain an elasticity between 0.7 and 0.9, meaning that their estimated consumption to household share of wealth elasticity should be lowered by approximately 20% when applied to &lt;em&gt;all&lt;/em&gt; non-durable goods and services.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amiti:2014ez (Importers, Exporters, and Exchange Rate Disconnect.)</title>
      <link>http://srg.spencerlyon.com/2016/10/04/amiti2014ez-importers-exporters-and-exchange-rate-disconnect./</link>
      <pubDate>Tue, 04 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/04/amiti2014ez-importers-exporters-and-exchange-rate-disconnect./</guid>
      <description>&lt;p&gt;&lt;p&gt;This paper aims to explain the low exchange rate pass-through for exporters. Exchange rate pass-through is the response in export prices to movement in the exchange rate.&lt;/p&gt;
&lt;h1 id=&#34;model&#34;&gt;Model&lt;/h1&gt;
&lt;p&gt;The theoretical model is not the focus of the authors&amp;rsquo; analysis in this paper. They write down a fairly complicated model, derive some of the equilibrium conditions, then use the implications of the equilibrium conditions as testable predictions they take to the data.&lt;/p&gt;
&lt;p&gt;I will attempt to summarize only the portions of the model that are necessary for understanding the testable predictions.&lt;/p&gt;
&lt;p&gt;The model is made up of two main components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;An oligopolistic competition model of variable markups following Atkeson and Burstein (2008). There are a continuum of sectors, each with a finite number of firms. Good are combined at the sector level using a CES technology. Then sector outputs are combined using another CES technology before being consumed by a representative household in each country. Household&amp;rsquo;s have preferences over the final consumption good and the labor they supply to firms.&lt;/li&gt;
&lt;li&gt;A model of the firm&amp;rsquo;s choice to import intermediate inputs at a fixed cost as in Halpern, Koren, and Szeidl (2011). Firms use a CRS Cobb-Douglass production function to combine a continuum of inputs. Each input is the CES aggregation of a domestic and foreign variety of the input. Foreign varieties have a multiplicative productivity advantage in the CES aggregator. Firms pay a firm specific fixed cost for each input variety they choose to import. Inputs are by sorted total productivity factor (combination of effect at the CES and Cobb Douglass levels), which together with the fixed costs makes the import policy have the form of a cutoff rule.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The oligopolistic competition portion of the model generates the fact that firms set prices at a constant markup over marginal cost. Furthermore, the markup is fully characterized by production function parameters and the firm&amp;rsquo;s market share at the sector-destination level.&lt;/p&gt;
&lt;p&gt;The intermediate input importing part of the model results in firms with larger total input costs or smaller fixed cost of importing have a larger import intensity. (defined as fraction of total variable costs spend on importing)&lt;/p&gt;
&lt;p&gt;Finally, the main theoretical result of the paper is that in any general equilibrium in this framework, the first-order approximation of the elasticity of destination-specific firm prices to the exchange rate (e.g. exchange-rate pass through) is affine in the importing intensity and market share.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;The main data used in the paper is from the National Bank of Belgium. It consists of a comprehensive panel of Belgian trade flows by firm at the product (CN 8 digit level). It includes exports by destination and imports by source country. This is combined with firm-level characteristics from the Belgian Business Registry. Data is measured annually between 2000 and 2008.&lt;/p&gt;
&lt;p&gt;The authors run a number of regressions. In each of them, the dependent variable is the log change in a firm&amp;rsquo;s export price of a good to a country. This is computed as the difference in the log of export value over export quantity.&lt;/p&gt;
&lt;p&gt;A key variable from the theory is the sector-destination-time market share of each firm. This is computed in the data as the total value exported by a firm to a destination divided by the total value exported from to that destination from the sector.&lt;/p&gt;
&lt;p&gt;The final key variable in the theory is the import intensity of the firm. This is defined as the total value of all non-euro zone imports over total variable costs.&lt;/p&gt;
&lt;p&gt;The authors also use data on the exchange rate and change in marginal costs.&lt;/p&gt;
&lt;h1 id=&#34;stylized-facts&#34;&gt;Stylized Facts&lt;/h1&gt;
&lt;p&gt;The data reveal a number of stylized facts about Belgian importers and exporters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import intensive firms (firms whose import intensity is above the median level of 4.2%) operate at a much larger scale than non import intensive firms.&lt;/li&gt;
&lt;li&gt;Import intensity is skewed towards zero, but has wide support and high dispersion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;The main empirical findings are summarized by the results of regressing the log change in export price on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change in exchange rate&lt;/li&gt;
&lt;li&gt;Lagged import intensity&lt;/li&gt;
&lt;li&gt;Lagged market share&lt;/li&gt;
&lt;li&gt;Interactions between change in exchange rate and both import intensity and market share&lt;/li&gt;
&lt;li&gt;Firm, destination fixed effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using variants of this specification, they document the following results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Including only the change in exchange rate and dummies, they find that exchange rate pass through (change in prices in response to change in exchange rate) is roughly 80%.&lt;/li&gt;
&lt;li&gt;Adding the interaction between the change in exchange rate and import intensity reveals that a 10 percent higher import intensity leads to a 6 percent lower pass-through. This is consistent with the model where pass- through is decreasing in import intensity.&lt;/li&gt;
&lt;li&gt;Controlling for changes in marginal costs causes the coefficient on the interaction of the exchange rate and import intensity to be cut in &amp;frac12;, but remain statistically significant at the 1% level. The coefficient on changes in marginal cost is almost twice as large as the import intensity term. This suggests that marginal cost is an important channel through which import intensity affects pass-through, but there is significant residual that operates through other channels. The theory indicates that the other channel is the markup channel.&lt;/li&gt;
&lt;li&gt;Regressing change in prices on changes in the exchange rate and interactions between import intensity and market share shows that both interaction terms have significant coefficients at the 1% level. Under the results of this regression, a small non-importer will have 96% pass through. A non-importing large firm with 75% market share will have pass-through of 73%. If additionally this large firm has an import intensity of 38%, the pas-through drops to 55%. This shows that variation in import intensity and market share explain a vast range of the variation in pass-through across firms.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ### Demand and Markups The authors consider a firm producing a differentiated good, within a particular sector, for a destination market, at a time t. Each firm is one of a finite number of producers within a sector (oligopolistic competition). Firm outputs are combined into sector level goods using a CES technology. These sector level outputs are again combined with a CES technology (different elasticity of substitution) before being consumed by a representative household. In this environment, the firm-sector-destination-time demand for a good is a function of: 1\. Relative preference (quality) for the firm 2\. The firm&#39;s price index 3\. The sector&#39;s price index 4\. The sector&#39;s demand shift (taken as given by the firm). Firm&#39;s end up charging a constant markup over marginal cost. The markup is fully characterized by CES elasticity parameters and the sector-destination-time market share for the firm. One of 3 main theoretical results, that is tested empirically, is that the markup and _elasticity of the markup with respect to firm price index_ are both increasing in market share. NOTE: From here to the end of this section is _really_ old Consumers of the good have nested CES preferences over the differentiated goods. A There are two countries (home and foreign) in the economy. We&#39;ll consider the domestic country; the economic environment in the foreign country is analogous. In each country there is a representative consumer that supplies labor to firms and has preferences over labor and a consumption good. The consumption good is produced by a competitive firm with CES technology over a continuum of sector-level outputs. Sector level goods are constructed using the output of a finite number of firm specific goods. These firms each produce a distinct good and operate in an oligopolistically com ### Production and Imported Inputs Production factors for differentiated goods producers are labor and an aggregated intermediate _input_. The intermediate _input_ is build by aggregating a continuum of intermediate _goods_ using a Cobb-Douglass technology. Each type of intermediate _good_ is the CES aggregation of imperfectly substitutable domestic and foreign _varieties_. A firm pays a firm specific fixed sunk cost (in labor units) to import each of the foreign varieties of the intermediate _good_. In the end, the firm has a total variable cost that is a function of: - Cost index for a non-importing firm - Cost reduction factor from importing - Firm productivity - Firm output Because of the variety-specific fixed costs, firms will not choose to import all goods. If goods are ordered according to the productivity-enhancement they provide, there will be a cutoff for which goods are imported by each firm. The import share for a firm is defined as the fraction of total variable costs that come from importing foreign varieties. --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kee:2016ej (Trade adjustment dynamics and the welfare gains from trade)</title>
      <link>http://srg.spencerlyon.com/2016/09/27/kee2016ej-trade-adjustment-dynamics-and-the-welfare-gains-from-trade/</link>
      <pubDate>Tue, 27 Sep 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/09/27/kee2016ej-trade-adjustment-dynamics-and-the-welfare-gains-from-trade/</guid>
      <description>&lt;p&gt;&lt;p&gt;A mostly empirical paper that examines the inputs used by Chinese firms to produce exported goods.&lt;/p&gt;
&lt;h2 id=&#34;dvar&#34;&gt;DVAR&lt;/h2&gt;
&lt;p&gt;The empirical analysis in this paper is centered around a variable named DVA, which stands for domestic value added in exports.&lt;/p&gt;
&lt;p&gt;To derive DVA, we start with total revenue. The authors break total revenue into the sum of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;profits&lt;/li&gt;
&lt;li&gt;labor costs&lt;/li&gt;
&lt;li&gt;capital costs&lt;/li&gt;
&lt;li&gt;materials from domestic sources&lt;/li&gt;
&lt;li&gt;materials from foreign sources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because intermediate good producers can get their inputs from China or foreign sources, the domestic and foreign materials sources are decomposed into Chinese and non-Chinese components.&lt;/p&gt;
&lt;p&gt;DVA is then defined as the sum of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;profits&lt;/li&gt;
&lt;li&gt;labor costs&lt;/li&gt;
&lt;li&gt;capital costs&lt;/li&gt;
&lt;li&gt;Chinese component of materials from domestic sources&lt;/li&gt;
&lt;li&gt;Chinese component of materials from foreign sources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The authors restrict their analysis to processing exporters, which means exporters who do not sell any of their final good in domestic markets. With this restriction the value of total exports is equal to total revenue. In this case DVA can be written as total exports less cost of imported materials plus and adjustment to account for foreign content materials from domestic materials.&lt;/p&gt;
&lt;p&gt;This is the form of DVA that is used throughout the paper. The important takeaway is that DAV is increasing in total exports and decreasing in cost of imported materials.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The authors use Chinese customs data that includes all exporting firms from 2000-2007. They apply three criterion to narrow down this universe to the final data set used in their analyses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Look at only processing firms&lt;/li&gt;
&lt;li&gt;Look at firms that operate in a single industry (difficult to decompose share of imports and exports within a firm across industries)&lt;/li&gt;
&lt;li&gt;Look at firms that aren&amp;rsquo;t &amp;quot;too extreme&amp;quot; in their importing and exporting behavior (an extreme exporter is a firm that imports strictly more than it exports &amp;ndash; selling the additional imported goods to other domestic producers)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In all analyses they normalize DVA by total exports for each firm. The resulting variable is named DVAR.&lt;/p&gt;
&lt;h2 id=&#34;main-findings&#34;&gt;Main findings&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll talk about 3 main findings:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Aggregate DVAR increased from near 45% to 55% from 2000 to 2007. This finding is also robust across industries, where 15 reported industries showed a similar evolution&lt;/li&gt;
&lt;li&gt;The increase in DVAR is driven by firms actively substituting imported materials for domestic materials, not by rising domestic production costs (e.g. labor and capital costs). This has policy implications. They do some regressions that show that changes in import tariffs for domestic, non-processing firms &amp;ndash; firms who sell input goods to the firms in our sample &amp;ndash; and rising FDI liberalization made significant contributions to rising DVAR.&lt;/li&gt;
&lt;li&gt;Restricting the analysis to processing exporters is not a bad estimate of total behavior. Recent work has used input-output tables to document facts about aggregate DVAR movement. The numbers reported here (using transactions level data, but only for processing firms) accounts for almost all the aggregate change.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brumm, Mikushin, Scheidegger and Schenk (2015) (Scalable high-dimensional dynamic stochastic economic modeling)</title>
      <link>http://srg.spencerlyon.com/2016/05/09/brumm-mikushin-scheidegger-and-schenk-2015-scalable-high-dimensional-dynamic-stochastic-economic-modeling/</link>
      <pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/05/09/brumm-mikushin-scheidegger-and-schenk-2015-scalable-high-dimensional-dynamic-stochastic-economic-modeling/</guid>
      <description>&lt;p&gt;An economist, a physicist, and two computer scientists walk into a bar...&lt;/p&gt;
&lt;p&gt;This is a computational paper that describes and algorithm featuring an adaptive sparse grid and discuss implementation details on a sophisticated HPC cluster.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;They provide examples of their algorithm and computation using a standard international real business cycle model. This is not the interesting part of the paper, so I will not focus on it here.&lt;/p&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h3 id=&#34;adaptive-sparse-grids&#34;&gt;Adaptive Sparse Grids&lt;/h3&gt;
&lt;p&gt;The first main contribution of this paper is to introduce economists to a specialized flavor of function approximation.&lt;/p&gt;
&lt;p&gt;The authors use familiar linear Splines; but do so on a sparse, adaptive, and hierarchical grid.&lt;/p&gt;
&lt;p&gt;By sparse I mean that the n-dimensional grid is not composed of the tensor product, or Cartesian product, of all univariate grids. This helps alleviate the curse of dimensionality.&lt;/p&gt;
&lt;p&gt;By adaptive I mean that the knot vector for the grid in each dimension will change as the solution algorithm for the economic problem proceeds. This helps preserve accuracy of function approximation routines when the grid is sparse.&lt;/p&gt;
&lt;p&gt;By hierarchical I mean that the grid for a particular level of refinement is a strict subset of the grid for all higher levels of refinement. This helps reduce computation costs as the basis functions for higher order terms only require a few additional function evaluations instead of a full basis matrix.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;The second main contribution of the paper is an algorithm that leverages this special interpolation scheme to solve high dimensional dynamic stochastic models.&lt;/p&gt;
&lt;p&gt;The main steps of the iterative portion of the algorithm are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with the coarsest refinement level &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;l&lt;/em&gt; = 1&lt;/span&gt; for all dimensions. Call the grid G. Choose a maximum refinement level&lt;/li&gt;
&lt;li&gt;Also Initialize &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;w&lt;/em&gt;&lt;/span&gt; to be the empty set&lt;/li&gt;
&lt;li&gt;While G and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt; are not the same (NOTE: at end of this loop explain the while goes until we are at Lmax or until we don&#39;t add refinement points)
&lt;ul&gt;
&lt;li&gt;For each grid point in &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt; \ &lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Solve the system of non-linear equations characterizing the optimal controls at that grid point (note you will need to interpolate over the current guess of the policy function) to obtain a new guess for the policy rule at that point&lt;/li&gt;
&lt;li&gt;If the distance between the old and new guess for the policy at the grid point is greater than some threshold, add the neighboring points at the next refinement level to G_new&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Set &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt; = &lt;em&gt;G&lt;/em&gt;&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt; = &lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt; ∪ &lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;w&lt;/em&gt;&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;w&lt;/em&gt; = ∅&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;l&lt;/em&gt; + =1&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Calculate the error for this iteration as the sup norm over the implied policy rule from the current iteration and the previous iteration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that this algorithm will start with very few points across the entire domain and iteratively add points only in regions where the policy rule has dramatic changes. This will naturally cause the grid to adapt and add more grid points in areas of the state space that feature non-linearities or discontinuities.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;This algorithm was implemented by coders who know their stuff! They build on cutting edge software and run their code on state of the art super computers in Switzerland.&lt;/p&gt;
&lt;p&gt;The results are impressive.&lt;/p&gt;
&lt;p&gt;On just one node of the super-computer, they were able to achieve a 30x speedup for their code by utilizing a GPU and multi-threaded parallelism.&lt;/p&gt;
&lt;p&gt;They were then able to scale that code from a single node to up to 2048 nodes to achieve a speedup on the order of 10,000x.&lt;/p&gt;
&lt;p&gt;They compare their algorithm to a non-adaptive sparse grid and find that the log 10 average Euler errors from their algorithm are smaller on average than the sparse grid case, but run time is up 2 to 3 orders of magnitude smaller.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Farias Saure Weintraub (2012) (An approximate dynamic programming approach to solving dynamic oligopoly models)</title>
      <link>http://srg.spencerlyon.com/2016/05/03/farias-saure-weintraub-2012-an-approximate-dynamic-programming-approach-to-solving-dynamic-oligopoly-models/</link>
      <pubDate>Tue, 03 May 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/05/03/farias-saure-weintraub-2012-an-approximate-dynamic-programming-approach-to-solving-dynamic-oligopoly-models/</guid>
      <description>&lt;p&gt;This paper builds on the approximate linear programming work of Farias and van Roy that I presented a few weeks ago and applies a version of that technique to a dynamic oligopoly model.&lt;/p&gt;
&lt;p&gt;The actual model is not novel to this research, so I will spend most of my time talking about the algorithm.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The model is set in discrete time and multiple firms compete in a single good market over an infinite horizon.&lt;/li&gt;
&lt;li&gt;Firms are identified by a firm specific state x that takes on an integer value between 0 and some upper limit xbar&lt;/li&gt;
&lt;li&gt;The aggregate state s is a histogram of the number of firms at each individual state (a vector of xbar+1 integers)&lt;/li&gt;
&lt;li&gt;The maximum number of incumbent firms is fixed at N. Each period there are &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;N&lt;/em&gt; − &lt;em&gt;s&lt;/em&gt;&lt;em&gt;u&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;)&lt;/span&gt; possible entrants. Entrants do not produce or earn profits in the first period.&lt;/li&gt;
&lt;li&gt;Incumbents choose an investment level that determines the probability of remaining in the same state, or moving up or down one step to a neighboring state in the next period.&lt;/li&gt;
&lt;li&gt;Each period the following events occur in this order:
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Incumbent firms draw a random sell-off cost and decide if they want to exit. If they stay, they make investment decisions.&lt;/li&gt;
&lt;li&gt;Each potential entrant draws a random entry cost and makes entry decision&lt;/li&gt;
&lt;li&gt;Incumbent firms compete in spot market and receive prices&lt;/li&gt;
&lt;li&gt;Exiting firms exit and receive sell off values&lt;/li&gt;
&lt;li&gt;Shocks are realized, each firm that stays transitions to a new state.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The equilibrium concept studied in this paper is a symmetric Markov perfect equilibrium. In this context a MPE is an investment/exit strategy for incumbents and an entry strategy for potential entrants such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;given that all other incumbents follow the exit/investment strategy, each incumbent does not want to deviate from that strategy&lt;/li&gt;
&lt;li&gt;For all states with a positive number of entrants, the cutoff entry value is equal to the expected discounted value of profits of entering the industry&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h3 id=&#34;naive-algorithm&#34;&gt;Naive algorithm&lt;/h3&gt;
&lt;p&gt;To understand the contribution of this paper, it is helpful to have a basic understanding of a naive &amp;quot;brute force&amp;quot; algorithm.&lt;/p&gt;
&lt;p&gt;The naive algorithm presented here is iterating on a best response operator and proceeds as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose some initial investment, exit, and entry policy&lt;/li&gt;
&lt;li&gt;Repeat the following until the policies are close enough:
&lt;ul&gt;
&lt;li&gt;Taking that N-1 players use the current guess for the policy, compute a best response for one agent. To do this you can apply standard dynamic programming algorithms&lt;/li&gt;
&lt;li&gt;Compute some notion of a norm between the best response and the current guess for the policy&lt;/li&gt;
&lt;li&gt;Set the current guess equal to the best response and continue if needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This algorithm is robust, but has one major drawback: the curse of dimensionality. For in a model with 20 incumbents and 20 individual states, there are over a thousand billion states that must be iterated over when computing the best response.&lt;/p&gt;
&lt;h3 id=&#34;approximate-dynamic-programming-algorithm&#34;&gt;Approximate dynamic programming algorithm&lt;/h3&gt;
&lt;p&gt;The authors of this paper make 4 modifications to the naive algorithm:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They form the linear program version of the dynamic programming problem to solve for the best response&lt;/li&gt;
&lt;li&gt;They apply approximate linear programming approach of de Farias and van Roy to reduce the number of variables the solver must find&lt;/li&gt;
&lt;li&gt;They use constraint sampling to only enforce a subset of the constraints of the linear program&lt;/li&gt;
&lt;li&gt;They choose basis functions that are especially well suited to their problem&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The authors do a few numerical experiments. The experiments all revolve around solving for the MPE exactly or approximately and then computing implied long run aggregates, such as average producer and consumer surplus, average share of ith largest firm, and average investment.&lt;/p&gt;
&lt;h4 id=&#34;exercise-1-small-model-exact-comparison&#34;&gt;Exercise 1: small model, exact comparison&lt;/h4&gt;
&lt;p&gt;For a relatively small version of the model (number of incumbents low), they are able to apply the naive algorithm and compute the true MPE. They also solve the model using their proposed algorithm and show that the aggregates are always within 8% of the true values, typically within 2%.&lt;/p&gt;
&lt;p&gt;Runtime for their algorithm is on the order of minutes in each case. With a max of 3 incumbents, runtime for the naive algorithm is on the order of seconds. When the max number of incumbents is 5, the naive algorithm takes a few hours.&lt;/p&gt;
&lt;h4 id=&#34;exercise-2-large-model-compare-oe&#34;&gt;Exercise 2: large model, compare OE&lt;/h4&gt;
&lt;p&gt;They also run a similar experiment for a much larger model where they compare a current state of the art algorithm to their proposed algorithm.&lt;/p&gt;
&lt;p&gt;They find that the aggregates are always within 13% of one another, but often within 5%.&lt;/p&gt;
&lt;p&gt;As the state of the art is also an approximation, this doesn&#39;t say much more than that both algorithms appear to approximate a similar thing.&lt;/p&gt;
&lt;p&gt;They comment that the runtime for a version of the model with 20 incumbents is a couple of hours. Unfortunately they don&#39;t elaborate on the difference in runtime between the algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timoshenko (2015) (Learning versus sunk costs explanations of export persistence)</title>
      <link>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-learning-versus-sunk-costs-explanations-of-export-persistence/</link>
      <pubDate>Tue, 26 Apr 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-learning-versus-sunk-costs-explanations-of-export-persistence/</guid>
      <description>&lt;p&gt;&lt;p&gt;This is a &lt;em&gt;mostly&lt;/em&gt; empirical paper that tries to decompose exporter persistence into sunk cost and learning components.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;Let firms be indexed by &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt;. Firms have two state variables: productivity &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;, &lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; that follows an AR(1) process and export experience &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;, &lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;. In &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;, &lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; is the number of consecutive periods a firm has exported coming into period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Without microfoudations, Timoshenko assumes that per-period sales are the Melitz result, multiplied by a function &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; of export experience:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;$S(z, A) = g(A) E P^{\sigma-1} \left( \frac{\sigma}{\sigma-1} \frac{\tau w}{\exp(z)} \right)^{1-\sigma}$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If a firm chooses to export, profits are given by &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;π&lt;/em&gt;(&lt;em&gt;z&lt;/em&gt;, &lt;em&gt;A&lt;/em&gt;)=&lt;em&gt;S&lt;/em&gt;(&lt;em&gt;z&lt;/em&gt;, &lt;em&gt;A&lt;/em&gt;)/&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; less a sunk market-entry cost if &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt; = 0&lt;/span&gt;. If a firm does not export, period profits are 0.&lt;/p&gt;
&lt;p&gt;Timoshenko assumes that &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt; is increasing in &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt;. She calls this the age-dependence (or learning) assumption and uses it to get empirical results later on.&lt;/p&gt;
&lt;p&gt;Export decision problem has a policy rule characterized by a pair of cutoff productivities &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt; &amp;gt; &lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;: a exit threshold &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt; and an entry threshold &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;. Firms enter when they get a productivity draw above &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;. They continue to export until productivity falls below &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt; because there is a hysteresis effect caused by &lt;span class=&#34;math inline&#34;&gt;$\frac{\partial g(A)}{\partial A} &amp;gt; 0$&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; is not a function of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt;, because by definition of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt; = 0&lt;/span&gt; for all firms considering entry.&lt;/p&gt;
&lt;h2 id=&#34;empirics&#34;&gt;Empirics&lt;/h2&gt;
&lt;p&gt;Timoshenko then does some empirical work to try to tease out the importance of the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; function on export sales and participation. Because there are sunk export costs in the model, she claims that considering both &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; and sunk costs isolates all learning effects into coefficients for &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To do this she starts from her reduced form theoretical model to build an empirical model. She controls for firm characteristics like location, domestic sales, investment, labor and materials expenditures, capital stock, wage rate, productivity, etc. She also builds in plant specific random effects. The result is an system of estimable equations that effectively regress the export decision rule on firm characteristics, exporter age dummies, and other controls.&lt;/p&gt;
&lt;h3 id=&#34;aside&#34;&gt;Aside&lt;/h3&gt;
&lt;p&gt;I don&amp;rsquo;t necessarily agree with her main claim. Without micro-foundations, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; doesn&amp;rsquo;t say anything about learning specifically. As it is written &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; is a simply a mechanism that causes sales and profits to increase with exporter age. This could be learning, more efficient production, more efficient transporting of goods, exogenous rise in foreign demand, market penetration/firm reputation in foreign market, etc. You might make an argument that the efficiency angles are a subset of learning: firms are &lt;em&gt;learning&lt;/em&gt; to be more efficient, but I believe separating different channels of learning is important.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;Using plant level data on Columbian firms in the 1980&amp;rsquo;s Timoshenko finds evidence that supports her age-dependence hypothesis. The findings are robust across industries. The punchline is that about &amp;frac12; of the value of the state dependence parameter is attributed to sunk costs and about &amp;frac12; is attributed to age-dependence. By state dependence I mean the impact of yesterday&amp;rsquo;s export status on today&amp;rsquo;s export decision.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timoshenko (2015) (Product switching in a model of learning)</title>
      <link>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-product-switching-in-a-model-of-learning/</link>
      <pubDate>Tue, 26 Apr 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-product-switching-in-a-model-of-learning/</guid>
      <description>&lt;p&gt;This paper documents facts about Brazilian manufacturing firms that switch their bundle of exported products over time. The author then builds a Melitz-style trade model that attempts to explain these facts.&lt;/p&gt;
&lt;h2 id=&#34;empirics&#34;&gt;Empirics&lt;/h2&gt;
&lt;p&gt;The author uses data product level data on Brazilian manufacturing firms to document several stylized facts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;72% of continuing exporters alter their product mix every year (add new exported products or drop existing exported products)&lt;/li&gt;
&lt;li&gt;83% of all Brazilian exports come from these product switching firms&lt;/li&gt;
&lt;li&gt;The proportion of exporters who do product switching falls with age in export market&lt;/li&gt;
&lt;li&gt;The frequency with which exporters engage in product switching falls with age&lt;/li&gt;
&lt;li&gt;The exit rate of product switching firms is lower than aggregate exit rate for all ages of exporter.&lt;/li&gt;
&lt;li&gt;Conditional on an exporter adding new products, over 1/2 of exporter products are added products&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;The model in this paper extends Melitz (2003) in two ways:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Consumers have firm specific demand shocks in their consumption aggregator&lt;/li&gt;
&lt;li&gt;Firms of a particular brand produce a finite, discrete number of products&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&#39;s the main points of the model:&lt;/p&gt;
&lt;h3 id=&#34;consumers&#34;&gt;Consumers&lt;/h3&gt;
&lt;p&gt;In each of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;N&lt;/em&gt;&lt;/span&gt; countries, there is a representative consumer that has log preferences over an aggregate consumption good.&lt;/p&gt;
&lt;p&gt;The aggregate good is a CES aggregated bundle of firm goods from all &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;N&lt;/em&gt;&lt;/span&gt; countries. Each firm good is hit with a time-varying, firm-and-import-country-specific demand shock.&lt;/p&gt;
&lt;p&gt;The firm goods are a CES aggregation of a finite number of differentiated products produced by the firm.&lt;/p&gt;
&lt;p&gt;Consumers inelastically supply a country specific fixed labor each period and earn labor income from firms. They also own firms in their country and retain all profits.&lt;/p&gt;
&lt;p&gt;Consumers take prices and wages as given maximize the utility of consuming the aggregate good subject to total expenditures being equal to labor income plus firm profits.&lt;/p&gt;
&lt;p&gt;The output of the consumer problem is a demand function for each product from each firm in each country.&lt;/p&gt;
&lt;p&gt;This demand function is a function of the consumer&#39;s income, the current demand shock for the firm and prices. Crucially, it can be inverted to give prices as a function of quantity and the demand shock.&lt;/p&gt;
&lt;h3 id=&#34;firms&#34;&gt;Firms&lt;/h3&gt;
&lt;p&gt;Firm&#39;s differ in their ability &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;φ&lt;/em&gt;&lt;/span&gt; to produce all their products. This ability or productivity is constant over time and across products and is drawn from a Pareto distribution.&lt;/p&gt;
&lt;p&gt;The demand shock in the consumer&#39;s CES aggregator is the sum of a constant firm specific component and a mean zero normally distributed iid shock drawn each period. The firm specific constant component is drawn from a normal distribution with known (to the firm) mean and variance. The prior beliefs of every firm are that the firm specific demand shock is drawn from its true distribution. Because signals and priors are normal, posterior beliefs are also normal and sufficient statistics are the mean and variance of the firm specific component.&lt;/p&gt;
&lt;p&gt;Firms choose the quantity of each product to be sold in each country, &lt;em&gt;before&lt;/em&gt; seeing the value of the demand shock for their firm in a given period. Consumers&#39; see this quantity and use their optimality conditions to declare the price they are willing to pay for the good. The firm observes this prices, which can be manipulated to reveal the current level of the demand shock. This acts as a signal about the firm specific constant component of the demand shock. Firms are Bayesian and update their beliefs about this component each period after observing the demand shock.&lt;/p&gt;
&lt;p&gt;Firms face fixed per-period cost of selling into each country. These costs increase with the number of products produced.&lt;/p&gt;
&lt;p&gt;Conditional on entering into a specific market, each period firms choose the number of products to be sold into the market as well as a quantity of each of these goods to maximize the expected value of their one period profits. Firm&#39;s operate in monopolistically competitive markets, so they understand how their quantity decision will impact the market clearing price. However, they do not know the current value of the demand shock, so they form expectations of the price using their current beliefs about the firm specific demand shock. The output of this problem is a quantity for each product as well as an expected profit for each profit.&lt;/p&gt;
&lt;p&gt;Firms continue to add products until the expected profits from adding more varieties (taking into account the fixed costs) are negative.&lt;/p&gt;
&lt;p&gt;Finally, firms face an entry and exit decision. A firm with productivity &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;φ&lt;/em&gt;&lt;/span&gt; has as state variables the sufficient statistics for their beliefs regarding the firm specific demand shock. They then choose whether or not to enter each market using the expected profit functions we just discussed.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Timoshenko looks at a symmetric, stationary equilibrium of this economy. Some properties of this equilibrium are are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The market participation policy is given by a cutoff threshold for the current expected value of the firm specific demand shock in terms of productive and other sufficient statistics. This cutoff is decreasing in productivity and increasing in the precision (inverse of variance).&lt;/li&gt;
&lt;li&gt;The quantity adjustment in response to seeing another signal about the demand shock is positive when the posterior mean is sufficiently high and negative when it is lower. The cutoff is a function of the prior mean and variance and the variance of the signal.&lt;/li&gt;
&lt;li&gt;Profits scale with quantities, so many high signals expands quantities and profits, and causes firms to add new products. A sequence of low signals has the opposite effect: lower quantities and profits leading to dropping products.&lt;/li&gt;
&lt;li&gt;Firms posterior precision increases deterministically, meaning asymptotically firms perfectly learn the firm specific component of the demand shocks. This generates age effects in quantity decisions and profits, which in turn generate age effects in adding and dropping products that match the data.&lt;/li&gt;
&lt;li&gt;If trade costs are lowered, the quantity of all current products is expanded. This is &lt;em&gt;not&lt;/em&gt; supported empirically. In the data, lower trade costs tend to have firms specialize more -- meaning they increase qualities of their most profitable products and scale back on quantities of marginal products.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;quantitative-results&#34;&gt;Quantitative results&lt;/h3&gt;
&lt;p&gt;A brief quantitative section is given. The main message is that the model generates more modest values of the number of exporters that engage in product switching, the age dependent survival rate of exporters, and the age dependence of product switching. This suggests that the learning mechanism in the model is significant and supported by the data, but not sufficient to fully explain product switching behavior of Brazilian firms.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>