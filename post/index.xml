<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Post-rsses on Sargent Reading Group Notes</title>
    <link>http://srg.spencerlyon.com/post/index.xml</link>
    <description>Recent content in Post-rsses on Sargent Reading Group Notes</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-EN</language>
    <managingEditor>spencer.lyon@stern.nyu.edu (Spencer Lyon)</managingEditor>
    <webMaster>spencer.lyon@stern.nyu.edu (Spencer Lyon)</webMaster>
    <copyright>(c) 2015 Spencer Lyon.</copyright>
    <lastBuildDate>Tue, 01 Nov 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://srg.spencerlyon.com/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Adam:2011ix (Adam, K., &amp; Marcet, A. (2011). Internal rationality, imperfect market knowledge and asset prices)</title>
      <link>http://srg.spencerlyon.com/2016/11/01/adam2011ix-adam-k.--marcet-a.-2011.-internal-rationality-imperfect-market-knowledge-and-asset-prices/</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/11/01/adam2011ix-adam-k.--marcet-a.-2011.-internal-rationality-imperfect-market-knowledge-and-asset-prices/</guid>
      <description>&lt;p&gt;&lt;p&gt;The basic idea in this paper is to separate the standard rationality requirements embedded in the rational expectations hypothesis into internal and external components. &lt;em&gt;Internal rationality&lt;/em&gt; means that the agents make fully optimal decisions given some well-defined subjective beliefs about payoff relevant variables. &lt;em&gt;External rationality&lt;/em&gt; requires that the probability distribution generated by agent subjective beliefs matches the true distribution of underlying variables.&lt;/p&gt;
&lt;p&gt;This paper will maintain the assumption of internal rationality, but not impose external rationality. To demonstrate the implications of this change, the authors use a simple Lucas asset pricing model to describe how the evolution of prices is different with and without extra rationality.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;Time is discrete. There are I infinitely-lived investor types, each with unit mass. Each agent is endowed with an equal share to an infinitely lived Lucas tree that stochastically yields consumable dividends Dt each period.&lt;/p&gt;
&lt;p&gt;Agents have risk-neutral, time separable preferences over streams of consumption. The discount factor and probability measure used by agents is type-specific. Each agent chooses a sequence of consumption and asset holdings to maximize the expected discounted value of consumption subject to budget constraint and that asset holdings are between 0 and some (large) upper bound each period. The budget constraint requires that consumption plus the cost of asset purchases is less than the sum of asset sales; dividend receipts; and an fixed, exogenous endowment of the consumption good. Each period the price of the asset is Pt.&lt;/p&gt;
&lt;p&gt;The non-standard part of the setup is that agents form beliefs over both realizations of asset prices and dividends. Under the REH, we typically assume that beliefs are over only dividend realizations and that agents know a mapping between dividends and prices when computing expectations.&lt;/p&gt;
&lt;p&gt;In this model, internal rationality is that each agent chooses consumption and asset holdings to maximize expected discounted utility subject to the constraints, taking their type&amp;rsquo;s probability measure as given.&lt;/p&gt;
&lt;h3 id=&#34;equilibrium&#34;&gt;Equilibrium&lt;/h3&gt;
&lt;p&gt;Agent&amp;rsquo;s optimality conditions are standard. An interior solution equates the current price with the discounted expected price plus dividends tomorrow. Without external rationality, agents have joint beliefs over prices and dividends, so they use this first order condition to derive the equilibrium price. Because discounting and expectations are formed type be type, the equilibrium price will be the maximum of this discounted expectation over all types.&lt;/p&gt;
&lt;p&gt;With external rationality (i.e. under standard rational expectations conditions) agents only have beliefs over the dividend process. They would use this first order condition together with the law of iterated expectations write the price today as the present discounted value of all future dividend payments.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s take a step back and think about this&amp;hellip;&lt;/p&gt;
&lt;p&gt;Note that we can consider external rationality as a special case of the model without external rationality. Specifically with external rationality agents are given a probability measure over dividends &lt;em&gt;and&lt;/em&gt;, implicitly, a mapping from histories of dividend realizations to prices. Knowledge of this function is &lt;em&gt;not&lt;/em&gt; an outcome of agent maximization (i.e. internal rationality), but rather the impact of a set of assumptions the modeler makes about what agents know about how the market operates. Given that economists haven&amp;rsquo;t found a mapping from dividend streams to prices, it seems reasonable to assume that agents don&amp;rsquo;t have this mapping either.&lt;/p&gt;
&lt;h3 id=&#34;internal-only-to-reh&#34;&gt;Internal only to REH&lt;/h3&gt;
&lt;p&gt;We now consider which assumptions are needed to go from the internal rationality only model to the model with both internal and external rationality. That is, we consider assumptions that allow our agents to write the current price as a expected discounted present value of future dividend payments.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;It is common knowledge that a single agent &amp;quot;sets the price&amp;quot; each period. I call this agent the marginal agent. This allows each agent to use &lt;em&gt;their&lt;/em&gt; own beliefs about who is marginal each period to write today&amp;rsquo;s price as the present discounted value of dividends.&lt;/li&gt;
&lt;li&gt;It is common knowledge that the last term in the infinite sum is zero, when the marginal agent&amp;rsquo;s beliefs and discount factor are applied each period. This gives agents information about the market in that all agents expect all future marginal agents to expect (and so on&amp;hellip;) that prices grow slower than the marginal discount factors. This is a no rational bubbles condition.&lt;/li&gt;
&lt;li&gt;All agents know which agent is marginal each period &lt;em&gt;and&lt;/em&gt; what the marginal agent&amp;rsquo;s discount factor and probability measure are. This allows all agents to write down the same infinite sum, that coincides with the equilibrium part.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Adam:2016hd (Adam, K., &amp; Marcet, A. &amp; Nicolini J. (2016). Stock Market Volatility and Learning.)</title>
      <link>http://srg.spencerlyon.com/2016/11/01/adam2016hd-adam-k.--marcet-a.--nicolini-j.-2016.-stock-market-volatility-and-learning./</link>
      <pubDate>Tue, 01 Nov 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/11/01/adam2016hd-adam-k.--marcet-a.--nicolini-j.-2016.-stock-market-volatility-and-learning./</guid>
      <description>&lt;p&gt;Builds on the internal rationality framework from last week to build a model of asset pricing that can explain 5 facts that have puzzled the literature at one time or another.&lt;/p&gt;
&lt;h2 id=&#34;facts&#34;&gt;Facts&lt;/h2&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Standard deviation of price dividend ratio is very high (about 1/2 the mean of the PD ratio)&lt;/li&gt;
&lt;li&gt;First-order quarterly autocorrelation of PD ratio is vary high&lt;/li&gt;
&lt;li&gt;Standard deviation of stock returns is almost 4 times as large as standard deviation of dividend growth&lt;/li&gt;
&lt;li&gt;PD ratio is good long run predictor of stock returns&lt;/li&gt;
&lt;li&gt;Equity premium puzzle: return on risky stocks is too high relative to bonds for standard models&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;There is a unit mass of infinitely lived investors, endowed with one unit of a stock that can be traded in a competitive market and that pays a per period dividend D in units of a perishable consumption good. The log growth rate of dividends follows an AR(1).&lt;/p&gt;
&lt;p&gt;Agents also receive a stochastic endowment of the consumption good. The feasibility constraint requires that consumption be equal to dividends plus the endowment. Following the consumption-based asset pricing literature they choose to model the consumption process directly instead of the income process. The log growth of consumption is also an AR(1). The innovations in consumption growth and dividend growth are correlated with coefficient equal to 0.2 (estimated in the data).&lt;/p&gt;
&lt;p&gt;In addition to trading shares of the dividend yielding risky asset, agents can also trade in a one period risk free bond.&lt;/p&gt;
&lt;p&gt;All agents discount the future at the same, constant rate.&lt;/p&gt;
&lt;p&gt;The objective of each consumer is to choose sequences of functions that map histories of observed prices, dividends, and endowments into consumption, stock holdings, and bond holdings to maximize the expected discounted utility of consumption; subject to a budget constraint that equates expenditure on consumption and stock and bond purchases to dividends, the endowment and bond returns.&lt;/p&gt;
&lt;p&gt;Each agent is allowed to have subjective beliefs over the joint evolution of prices, dividends, and endowments. Agents behave fully rationally, given these beliefs. The only difference between the setup here and the classic setup is that agents form beliefs over prices instead of being assumed to have a mapping between histories of dividends and endowments into current prices. These subjective beliefs will be updated over time and are crucial for the model to explain the facts in the data.&lt;/p&gt;
&lt;h3 id=&#34;solution&#34;&gt;Solution&lt;/h3&gt;
&lt;p&gt;The agents first order necessary conditions for stocks and bonds are standard.&lt;/p&gt;
&lt;p&gt;A key equilibrium result is the pricing function that relates current dividends and expected price and dividend growth into current prices.&lt;/p&gt;
&lt;p&gt;This function is that prices today are equal to risk adjusted expected dividend growth, divided by one minus risk adjusted expected price growth, times the discount factor, times dividends. It is crucial that todays price is increasing the divided flow, risk-adjusted expectations about dividend growth, and risk-adjusted expectations about price growth.&lt;/p&gt;
&lt;p&gt;Under the rational expectations hypothesis, the expectation of both of these growth rates is constant. This results in growth rate of prices to exactly equal the growth rate of dividends. It is for this reason that the standard rational expectations model fails to match the first 4 facts I gave before. The model also generates a low risk premium, so it misses all 5 facts.&lt;/p&gt;
&lt;p&gt;However, when beliefs are subjective the expected risk-adjusted growth rates are not constant over time. The authors argue that the important component of their model is fluctuations in the expectation of price growth. To test this, they assume that investors use the true probability measure for divided growth, but hold subjective beliefs over price growth. This is the same as assuming agents still cannot map perfectly from dividend and endowment realizations into prices.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The baseline parameterization of the proposed model is able to quantitatively match all the facts except the risk premium. In this model the CRRA parameter is set to 5. If they allow this to float to 80, they are also able to match the risk premium fact.&lt;/p&gt;
&lt;p&gt;The question is, how does it happen?&lt;/p&gt;
&lt;p&gt;The key mechanism is a feedback loop between expectations about price growth and the realization of price growth. Here&#39;s a rough sketch of how it works&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Expected price growth exhibits both momentum and mean reversion to the rational expectants &amp;quot;fundamental level&amp;quot; of prices&lt;/li&gt;
&lt;li&gt;This momentum means that beliefs have a tendency to increase further following an initial increase whenever beliefs are at or below their fundamental value.&lt;/li&gt;
&lt;li&gt;Mean reversion means that beliefs can&#39;t stay away from the rational expectations level forever.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These facts together cause the price dividend ratio to fluctuate around its rational expectation value. This allows the model to explain the observed volatility and serial correlation of the PD ratio (facts 1 and 2). This behavior also causes stock returns to be more volatile than dividend growth; which explains fact 3. Finally, serial correlation and mean-reversion in the PD ratio give rise to excess return predictability -- fact 4 (NOTE: the notion of predictability is explained in another paper by Cochrane that I didn&#39;t study).&lt;/p&gt;
&lt;p&gt;The model still fails, however to generate a high enough risk premium. In order to match this fact, they authors have to adjust risk aversion. They do so by cranking up the CRRA parameter. I believe that using recursive preferences and adjusting the inter temporal elasticity of substitution is another way to generate excess returns without resorting to very high levels of risk aversion.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Baley:2016wr (Firm uncertainty cycles and the propagation of nominal shocks)</title>
      <link>http://srg.spencerlyon.com/2016/10/25/baley2016wr-firm-uncertainty-cycles-and-the-propagation-of-nominal-shocks/</link>
      <pubDate>Tue, 25 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/25/baley2016wr-firm-uncertainty-cycles-and-the-propagation-of-nominal-shocks/</guid>
      <description>&lt;p&gt;&lt;p&gt;This paper develops a model with menu costs for adjusting prices and imperfect information about idiosyncratic productivity shocks. They conduct monetary policy experiments and conclude that the distribution of firm level uncertainty is important for the propagation of monetary shocks.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;In this model time is continuous. There is a representative consumer, a continuum of monopolistically competitive firms, and a monetary authority.&lt;/p&gt;
&lt;p&gt;In the baseline model, the monetary authority keeps the money supply fixed at its initial level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The representative consumer&lt;/strong&gt; has log preferences over consumption and money holdings, with linear disutility from labor. The household discounts the future at a constant rate r. Consumption is the CES aggregation of firm specific goods, each multiplied by a firm specific quality shock. The lifetime budget constraint of the consumer says that the value of consumption expenditures plus the cost of holding money, less labor earnings and firm profits is no greater than the initial money supply.&lt;/p&gt;
&lt;p&gt;There are a continuum of &lt;strong&gt;firms&lt;/strong&gt; who each product and sell their product in a monopolistically competitive market. Each firm has access to a linear technology that produces one unit of output for each unit of labor, divided by the stochastic quality of the good (meaning a higher quality good requires more labor to produce).&lt;/p&gt;
&lt;p&gt;This is the same quality that appears in the CES aggregator for consumption. The log of the quality shock follows a jump-diffusion process, without drift. This means that &lt;code&gt;da(z) = sigma1 dW + sigma2 u dQ&lt;/code&gt;, where W is a Weiner process and u Q is a poisson process with standard normal innovations. It is assumed that sigma1 is much smaller than sigma2, such that when the poisson process jumps there is a large shock to the quality of the firm&amp;rsquo;s product.&lt;/p&gt;
&lt;p&gt;Firms do not observe the quality directly and cannot learn about it using wage costs. The only information they receive about their quality is a noisy signal and the information about when the poisson process jumps. The evolution of the signal follows &lt;code&gt;ds = a dt + gamma dZ&lt;/code&gt;, where Z is another independent Weiner process.&lt;/p&gt;
&lt;p&gt;Firm prices are subject nominal rigidities in which firms can only change their price if they pay a fixed menu cost &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The flow profit to a firm is given by the demand for the good, multiplied by the price less the wage bill.&lt;/p&gt;
&lt;p&gt;Firms use the discount rate of the household (because households own firms) and seek to maximize the expected discounted profit streams by choosing a sequence of prices and stopping times for price adjustments. When firms adjust prices, it is optimal to set them to a constant markup over marginal costs. Each period the firm does not adjust its price, there is a gap between the optimal markup in a frictionless economy (i.e. without both price and information frictions) and the markup being used by the firm. The equilibrium flow of profits can be expressed as minus a constant times this markup gap.&lt;/p&gt;
&lt;p&gt;Because firms cannot directly observe the quality of their goods, they do not know the true value of the markup gap each period. Instead, they use the signals about the quality shock to form a signal extraction problem. A main contribution of the paper is the extension of the Kalman-Bucy filter to the environment where the hidden state follows a jump-diffusion process. One key output of the system of filtering equations is that innovations in the estimate of the markup gap are more volatile when there is high uncertainty about the estimate &amp;ndash; e.g. when the variance of the estimate is high. An implication of this is that when uncertainty is high, firms place higher weight on their signals than they do on the current estimate when updating beliefs. In this scenario, the learning rate is higher, but also noisier.&lt;/p&gt;
&lt;p&gt;The optimal stopping time for firms is characterized by an inaction region &amp;ndash; as long as the filtered estimate for the markup gap is within certain bounds, the firm does not update prices. Once the price touches one of the borders, it immediately adjusts prices to set the estimated markup gap to zero.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Here are some key results about the steady state equilibrium of this economy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The filtering equation for updating uncertainty (the variance of the state estimate) produces cycles of uncertainty for the firm. There is a deterministic component that causes uncertainty to fall to it&amp;rsquo;s minimum level (the volatility of the diffusion component of quality shocks) and a stochastic component that causes it to rise whenever Poisson process jumps. The time series for a simulated path has a saw-tooth pattern where sharp increases are followed by gradual declines in uncertainty.&lt;/li&gt;
&lt;li&gt;In times when uncertainty is high, the innovations to the filtered estimate of the markup gap are more volatile. This generates cycles in firm markup behavior where in times of high uncertainty firms choose to update prices more frequently. These cycles in uncertainty and pricing behavior are idiosyncratic because they are driven by idiosyncratic evolution of Brownian Motion and Poisson processes.&lt;/li&gt;
&lt;li&gt;The idiosyncratic cycles allow the authors to look at how the &lt;em&gt;distribution&lt;/em&gt; of uncertainty relates to the response to monetary shocks. To do this they perform an experiment where the monetary authority does a one-time un-anticipated increase in the money supply by a known amount. When the monetary shock occurs, estimates of the markup gap are updated immediately. But, a firm&amp;rsquo;s price will only get update when its estimate leaves its inaction region. As long as some firms&amp;rsquo; estimates remain in the inaction region, aggregate output will be different from its post-shock steady state value. As a summary statistic, they compute the total output effect, which is the integral over all time of the output gap relative to the steady state. They find that in the economy with information frictions, the output effect is larger up to 7 times larger and has a half life up to 5.3 times longer half life than in the perfect information economy. In this sense the authors claim that the distribution of firm level uncertainty is an important consideration when trying to evaluate the effects of monetary policy actions.&lt;/li&gt;
&lt;/ul&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ikegami:2016ek (Poverty Traps and the Social Protection Paradox)</title>
      <link>http://srg.spencerlyon.com/2016/10/18/ikegami2016ek-poverty-traps-and-the-social-protection-paradox/</link>
      <pubDate>Tue, 18 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/18/ikegami2016ek-poverty-traps-and-the-social-protection-paradox/</guid>
      <description>&lt;p&gt;This paper presents and analyzes a stylized model of poverty traps in developing economies.&lt;/p&gt;
&lt;h2 id=&#34;baseline-model&#34;&gt;Baseline model&lt;/h2&gt;
&lt;p&gt;In the baseline model there are a finite number of agents.&lt;/p&gt;
&lt;p&gt;Agent&#39;s are characterized by a constant level of innate ability &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;α&lt;/em&gt;&lt;/span&gt; and an evolving stock of capital &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Each period, agents choose which of two DRS production technologies they wish to operate in that period. Switching technologies is costless. Both technologies have innate ability multiplied by physical capital raised to a power less than one. The more productive technology has a higher exponent, but requires payment of a fixed cost to operate.&lt;/p&gt;
&lt;p&gt;Agent&#39;s have CRRA preferences over consumption.&lt;/p&gt;
&lt;p&gt;Agents choose consumption and technology each period to maximize the expected discounted value of lifetime utility from consumption, subject to a few constraints:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Consumption can be no more than capital stock plus production output&lt;/li&gt;
&lt;li&gt;The capital stock is always non-negative&lt;/li&gt;
&lt;li&gt;Next period capital stock is the unconsumed portion of resources multiplied by an asset shock less depreciation. When this shock is less than unity, some capital is destroyed. The shocks are drawn iid from a known distribution.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model-solution&#34;&gt;Model Solution&lt;/h2&gt;
&lt;p&gt;The dynamics induced by agents&#39; optimal behavior can be understood by considering two interacting effects.&lt;/p&gt;
&lt;p&gt;First, the choice of production technology is governed by a cutoff rule for capital as a function of innate ability. If an agent&#39;s capital stock is above this threshold, they will attempt to accumulate capital so that they can employ the high productivity technology. Otherwise the agent will only pursue the low technology and accumulate the relatively smaller stock of capital needed to operate that technology optimally.&lt;/p&gt;
&lt;p&gt;Second, the state space can be partitioned into three regions along the innate ability dimension&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The unskilled worker region, where regardless of the amount of capital agents always find it optimal to use the low productivity technology. For each innate ability level, there is a unique optimal capital stock to target. Poverty is defined as having capital stock equal to this level.&lt;/li&gt;
&lt;li&gt;A high skill region where for all levels of capital agents prefer to operate the high productivity technology.&lt;/li&gt;
&lt;li&gt;An intermediate region where depending on the current capital stock and sequence of asset shocks, agents may choose to operate either technology.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These regions define two forms of poverty trap:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Low skill agents who will always be in poverty&lt;/li&gt;
&lt;li&gt;Middle skill agents who are vulnerable to being pushed into poverty if they receive sufficiently unfavorable asset shocks.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;policy&#34;&gt;Policy&lt;/h2&gt;
&lt;p&gt;The authors use this framework to analyze a few competing forms of government intervention. They analyze the effectiveness of each policy using a simulation experiment. In each experiment they randomly initialize 300 agents to be 25% in the low and high skill region and 50% in the intermediate region. The capital stock is initialized by independent draws from uniform distribution on [0, 10]. They they simulate the model forward under a given policy for 50 periods and track the distribution of agents.&lt;/p&gt;
&lt;p&gt;As a baseline, we first consider autarky, or no government transfer program. The results of the simulation are a clear increase in the poverty level relative to the initial conditions. At the start, about 60% of the population chose to operate the high productivity technology. At then end of the simulation this has dropped to 40%.&lt;/p&gt;
&lt;p&gt;The first policy considered is a progressive policy that targets the poorest agents in the population. All agents below the poverty level are given a transfer that brings them exactly to the poverty line. If there are insufficient government funds, each agent is given a share of total government resources proportional to their distance from the poverty line. Agents do not anticipate the transfers. The results of simulating in this environment are qualitatively identical to autarky.&lt;/p&gt;
&lt;p&gt;The second policy targets the middle skill agents near the cutoff rule for switching between production technologies. Specifically, if an agent starts the period in the high technology region and recipes a poor enough asset shock to move to the low region -- the government provides a transfer that brings the agent exactly back to the cutoff level. Again the transfer is unanticipated. The simulation results here are quite different. Aggregate output rises by 10% and poverty falls from 55% to 25% -- meaning 75% of agents operate the high productivity technology.&lt;/p&gt;
&lt;p&gt;The difference between these two policies brings up an ethical issue regarding which subset of agents government policies should target. The authors provide some discussion, but leave it as an open ended question.&lt;/p&gt;
&lt;p&gt;The final experiment also targets the middle skill agents near the cutoff, but this time the transfers are anticipated. The anticipation brings about two competing moral hazard forces:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;The positive force is that when agents know they will receive a transfer if their investment is subject to poor shocks, they choose to invest more.&lt;/li&gt;
&lt;li&gt;The negative force is that agents would like to remain as close to the cutoff as possible so they can get the transfer more often.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The results of the simulation under these conditions falls between the two previous examples. The authors don&#39;t report numbers, so I can&#39;t be precise. However, from the figures the main takeaway is that agents in the middle skill region who are close to the production cutoff are now much less likely to end up operating the low productivity technology in the long run.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kaplan:2016fe (Non-durable Consumption and Housing Net Worth in the Great Recession: Evidence from Easily Accessible Data.)</title>
      <link>http://srg.spencerlyon.com/2016/10/11/kaplan2016fe-non-durable-consumption-and-housing-net-worth-in-the-great-recession-evidence-from-easily-accessible-data./</link>
      <pubDate>Tue, 11 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/11/kaplan2016fe-non-durable-consumption-and-housing-net-worth-in-the-great-recession-evidence-from-easily-accessible-data./</guid>
      <description>&lt;p&gt;&lt;p&gt;In 2013; Mian, Rao, Sufi used proprietary data on the US housing market (obtained from Core Logic) and personal consumption expenditures (obtained from master card) from 2006-2009 to estimate that the elasticity of consumption expenditures to changes in the housing share of household net worth.&lt;/p&gt;
&lt;p&gt;This paper replicates the main results from Mian, Rao, and Sufi using data that is more easily accessed by economists in academia.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The authors proxy the housing data from Core Logic, using housing data from Zillow. The data is freely downloadable from the Zillow website.&lt;/p&gt;
&lt;p&gt;Instead of the mastercard data, the authors use data from the Kilts Neilsen scanner retail survey. This survey includes weekly price and quantity levels for sales at the bar code level for about 40,000 US stores from 2006-2009 (the panel is still ongoing and currently runs to 2014). KMV estimate that this subset is approximately 40% of aggregate US consumption on non-durables.&lt;/p&gt;
&lt;h2 id=&#34;replication&#34;&gt;Replication&lt;/h2&gt;
&lt;p&gt;Using the Core logic and mastercard data to, MRS report an elasticity of consumption expenditures to changes in the housing share of household net worth between 0.33 and 0.36 (depending on the controls in the regression and regression technique &amp;ndash; OLS vs IV2SLS).&lt;/p&gt;
&lt;p&gt;Using the Zillow and Kilts Neilsen data, KVM report an elasticity between 0.24 and 0.36.&lt;/p&gt;
&lt;p&gt;The similarity of these findings despite the very different data sets is encouraging.&lt;/p&gt;
&lt;h2 id=&#34;new-contributions&#34;&gt;New contributions&lt;/h2&gt;
&lt;p&gt;In addition to replicating the results from MRS, KMV have 3 main findings:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They show that the interaction between the fall in local house prices and the size of initial leverage is not statistically significant, after controlling for the direct impact of house price changes.&lt;/li&gt;
&lt;li&gt;They separate the price and quantity components in the fall in consumption expenditure during the great recession. They construct a proxy measure of the quantity of household expenditure by aggregating quantity sold from all stores at the product level, and then multiplying by an average price for the product. When this is used as the dependent variable in the regression, elasticities are approximately 20% lower.&lt;/li&gt;
&lt;li&gt;They use the Diary Survey of the Consumer Expenditure Survey to estimate the elasticity of total non-durable goods and survives to the counter part found in the Kilts Nielsen dataset. They obtain an elasticity between 0.7 and 0.9, meaning that their estimated consumption to household share of wealth elasticity should be lowered by approximately 20% when applied to &lt;em&gt;all&lt;/em&gt; non-durable goods and services.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Amiti:2014ez (Importers, Exporters, and Exchange Rate Disconnect.)</title>
      <link>http://srg.spencerlyon.com/2016/10/04/amiti2014ez-importers-exporters-and-exchange-rate-disconnect./</link>
      <pubDate>Tue, 04 Oct 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/10/04/amiti2014ez-importers-exporters-and-exchange-rate-disconnect./</guid>
      <description>&lt;p&gt;&lt;p&gt;This paper aims to explain the low exchange rate pass-through for exporters. Exchange rate pass-through is the response in export prices to movement in the exchange rate.&lt;/p&gt;
&lt;h1 id=&#34;model&#34;&gt;Model&lt;/h1&gt;
&lt;p&gt;The theoretical model is not the focus of the authors&amp;rsquo; analysis in this paper. They write down a fairly complicated model, derive some of the equilibrium conditions, then use the implications of the equilibrium conditions as testable predictions they take to the data.&lt;/p&gt;
&lt;p&gt;I will attempt to summarize only the portions of the model that are necessary for understanding the testable predictions.&lt;/p&gt;
&lt;p&gt;The model is made up of two main components:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;An oligopolistic competition model of variable markups following Atkeson and Burstein (2008). There are a continuum of sectors, each with a finite number of firms. Good are combined at the sector level using a CES technology. Then sector outputs are combined using another CES technology before being consumed by a representative household in each country. Household&amp;rsquo;s have preferences over the final consumption good and the labor they supply to firms.&lt;/li&gt;
&lt;li&gt;A model of the firm&amp;rsquo;s choice to import intermediate inputs at a fixed cost as in Halpern, Koren, and Szeidl (2011). Firms use a CRS Cobb-Douglass production function to combine a continuum of inputs. Each input is the CES aggregation of a domestic and foreign variety of the input. Foreign varieties have a multiplicative productivity advantage in the CES aggregator. Firms pay a firm specific fixed cost for each input variety they choose to import. Inputs are by sorted total productivity factor (combination of effect at the CES and Cobb Douglass levels), which together with the fixed costs makes the import policy have the form of a cutoff rule.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The oligopolistic competition portion of the model generates the fact that firms set prices at a constant markup over marginal cost. Furthermore, the markup is fully characterized by production function parameters and the firm&amp;rsquo;s market share at the sector-destination level.&lt;/p&gt;
&lt;p&gt;The intermediate input importing part of the model results in firms with larger total input costs or smaller fixed cost of importing have a larger import intensity. (defined as fraction of total variable costs spend on importing)&lt;/p&gt;
&lt;p&gt;Finally, the main theoretical result of the paper is that in any general equilibrium in this framework, the first-order approximation of the elasticity of destination-specific firm prices to the exchange rate (e.g. exchange-rate pass through) is affine in the importing intensity and market share.&lt;/p&gt;
&lt;h1 id=&#34;data&#34;&gt;Data&lt;/h1&gt;
&lt;p&gt;The main data used in the paper is from the National Bank of Belgium. It consists of a comprehensive panel of Belgian trade flows by firm at the product (CN 8 digit level). It includes exports by destination and imports by source country. This is combined with firm-level characteristics from the Belgian Business Registry. Data is measured annually between 2000 and 2008.&lt;/p&gt;
&lt;p&gt;The authors run a number of regressions. In each of them, the dependent variable is the log change in a firm&amp;rsquo;s export price of a good to a country. This is computed as the difference in the log of export value over export quantity.&lt;/p&gt;
&lt;p&gt;A key variable from the theory is the sector-destination-time market share of each firm. This is computed in the data as the total value exported by a firm to a destination divided by the total value exported from to that destination from the sector.&lt;/p&gt;
&lt;p&gt;The final key variable in the theory is the import intensity of the firm. This is defined as the total value of all non-euro zone imports over total variable costs.&lt;/p&gt;
&lt;p&gt;The authors also use data on the exchange rate and change in marginal costs.&lt;/p&gt;
&lt;h1 id=&#34;stylized-facts&#34;&gt;Stylized Facts&lt;/h1&gt;
&lt;p&gt;The data reveal a number of stylized facts about Belgian importers and exporters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Import intensive firms (firms whose import intensity is above the median level of 4.2%) operate at a much larger scale than non import intensive firms.&lt;/li&gt;
&lt;li&gt;Import intensity is skewed towards zero, but has wide support and high dispersion.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;results&#34;&gt;Results&lt;/h1&gt;
&lt;p&gt;The main empirical findings are summarized by the results of regressing the log change in export price on&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Change in exchange rate&lt;/li&gt;
&lt;li&gt;Lagged import intensity&lt;/li&gt;
&lt;li&gt;Lagged market share&lt;/li&gt;
&lt;li&gt;Interactions between change in exchange rate and both import intensity and market share&lt;/li&gt;
&lt;li&gt;Firm, destination fixed effects&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Using variants of this specification, they document the following results:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Including only the change in exchange rate and dummies, they find that exchange rate pass through (change in prices in response to change in exchange rate) is roughly 80%.&lt;/li&gt;
&lt;li&gt;Adding the interaction between the change in exchange rate and import intensity reveals that a 10 percent higher import intensity leads to a 6 percent lower pass-through. This is consistent with the model where pass- through is decreasing in import intensity.&lt;/li&gt;
&lt;li&gt;Controlling for changes in marginal costs causes the coefficient on the interaction of the exchange rate and import intensity to be cut in &amp;frac12;, but remain statistically significant at the 1% level. The coefficient on changes in marginal cost is almost twice as large as the import intensity term. This suggests that marginal cost is an important channel through which import intensity affects pass-through, but there is significant residual that operates through other channels. The theory indicates that the other channel is the markup channel.&lt;/li&gt;
&lt;li&gt;Regressing change in prices on changes in the exchange rate and interactions between import intensity and market share shows that both interaction terms have significant coefficients at the 1% level. Under the results of this regression, a small non-importer will have 96% pass through. A non-importing large firm with 75% market share will have pass-through of 73%. If additionally this large firm has an import intensity of 38%, the pas-through drops to 55%. This shows that variation in import intensity and market share explain a vast range of the variation in pass-through across firms.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- ### Demand and Markups The authors consider a firm producing a differentiated good, within a particular sector, for a destination market, at a time t. Each firm is one of a finite number of producers within a sector (oligopolistic competition). Firm outputs are combined into sector level goods using a CES technology. These sector level outputs are again combined with a CES technology (different elasticity of substitution) before being consumed by a representative household. In this environment, the firm-sector-destination-time demand for a good is a function of: 1\. Relative preference (quality) for the firm 2\. The firm&#39;s price index 3\. The sector&#39;s price index 4\. The sector&#39;s demand shift (taken as given by the firm). Firm&#39;s end up charging a constant markup over marginal cost. The markup is fully characterized by CES elasticity parameters and the sector-destination-time market share for the firm. One of 3 main theoretical results, that is tested empirically, is that the markup and _elasticity of the markup with respect to firm price index_ are both increasing in market share. NOTE: From here to the end of this section is _really_ old Consumers of the good have nested CES preferences over the differentiated goods. A There are two countries (home and foreign) in the economy. We&#39;ll consider the domestic country; the economic environment in the foreign country is analogous. In each country there is a representative consumer that supplies labor to firms and has preferences over labor and a consumption good. The consumption good is produced by a competitive firm with CES technology over a continuum of sector-level outputs. Sector level goods are constructed using the output of a finite number of firm specific goods. These firms each produce a distinct good and operate in an oligopolistically com ### Production and Imported Inputs Production factors for differentiated goods producers are labor and an aggregated intermediate _input_. The intermediate _input_ is build by aggregating a continuum of intermediate _goods_ using a Cobb-Douglass technology. Each type of intermediate _good_ is the CES aggregation of imperfectly substitutable domestic and foreign _varieties_. A firm pays a firm specific fixed sunk cost (in labor units) to import each of the foreign varieties of the intermediate _good_. In the end, the firm has a total variable cost that is a function of: - Cost index for a non-importing firm - Cost reduction factor from importing - Firm productivity - Firm output Because of the variety-specific fixed costs, firms will not choose to import all goods. If goods are ordered according to the productivity-enhancement they provide, there will be a cutoff for which goods are imported by each firm. The import share for a firm is defined as the fraction of total variable costs that come from importing foreign varieties. --&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kee:2016ej (Trade adjustment dynamics and the welfare gains from trade)</title>
      <link>http://srg.spencerlyon.com/2016/09/27/kee2016ej-trade-adjustment-dynamics-and-the-welfare-gains-from-trade/</link>
      <pubDate>Tue, 27 Sep 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/09/27/kee2016ej-trade-adjustment-dynamics-and-the-welfare-gains-from-trade/</guid>
      <description>&lt;p&gt;&lt;p&gt;A mostly empirical paper that examines the inputs used by Chinese firms to produce exported goods.&lt;/p&gt;
&lt;h2 id=&#34;dvar&#34;&gt;DVAR&lt;/h2&gt;
&lt;p&gt;The empirical analysis in this paper is centered around a variable named DVA, which stands for domestic value added in exports.&lt;/p&gt;
&lt;p&gt;To derive DVA, we start with total revenue. The authors break total revenue into the sum of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;profits&lt;/li&gt;
&lt;li&gt;labor costs&lt;/li&gt;
&lt;li&gt;capital costs&lt;/li&gt;
&lt;li&gt;materials from domestic sources&lt;/li&gt;
&lt;li&gt;materials from foreign sources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Because intermediate good producers can get their inputs from China or foreign sources, the domestic and foreign materials sources are decomposed into Chinese and non-Chinese components.&lt;/p&gt;
&lt;p&gt;DVA is then defined as the sum of&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;profits&lt;/li&gt;
&lt;li&gt;labor costs&lt;/li&gt;
&lt;li&gt;capital costs&lt;/li&gt;
&lt;li&gt;Chinese component of materials from domestic sources&lt;/li&gt;
&lt;li&gt;Chinese component of materials from foreign sources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The authors restrict their analysis to processing exporters, which means exporters who do not sell any of their final good in domestic markets. With this restriction the value of total exports is equal to total revenue. In this case DVA can be written as total exports less cost of imported materials plus and adjustment to account for foreign content materials from domestic materials.&lt;/p&gt;
&lt;p&gt;This is the form of DVA that is used throughout the paper. The important takeaway is that DAV is increasing in total exports and decreasing in cost of imported materials.&lt;/p&gt;
&lt;h2 id=&#34;data&#34;&gt;Data&lt;/h2&gt;
&lt;p&gt;The authors use Chinese customs data that includes all exporting firms from 2000-2007. They apply three criterion to narrow down this universe to the final data set used in their analyses:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Look at only processing firms&lt;/li&gt;
&lt;li&gt;Look at firms that operate in a single industry (difficult to decompose share of imports and exports within a firm across industries)&lt;/li&gt;
&lt;li&gt;Look at firms that aren&amp;rsquo;t &amp;quot;too extreme&amp;quot; in their importing and exporting behavior (an extreme exporter is a firm that imports strictly more than it exports &amp;ndash; selling the additional imported goods to other domestic producers)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In all analyses they normalize DVA by total exports for each firm. The resulting variable is named DVAR.&lt;/p&gt;
&lt;h2 id=&#34;main-findings&#34;&gt;Main findings&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll talk about 3 main findings:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Aggregate DVAR increased from near 45% to 55% from 2000 to 2007. This finding is also robust across industries, where 15 reported industries showed a similar evolution&lt;/li&gt;
&lt;li&gt;The increase in DVAR is driven by firms actively substituting imported materials for domestic materials, not by rising domestic production costs (e.g. labor and capital costs). This has policy implications. They do some regressions that show that changes in import tariffs for domestic, non-processing firms &amp;ndash; firms who sell input goods to the firms in our sample &amp;ndash; and rising FDI liberalization made significant contributions to rising DVAR.&lt;/li&gt;
&lt;li&gt;Restricting the analysis to processing exporters is not a bad estimate of total behavior. Recent work has used input-output tables to document facts about aggregate DVAR movement. The numbers reported here (using transactions level data, but only for processing firms) accounts for almost all the aggregate change.&lt;/li&gt;
&lt;/ol&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brumm, Mikushin, Scheidegger and Schenk (2015) (Scalable high-dimensional dynamic stochastic economic modeling)</title>
      <link>http://srg.spencerlyon.com/2016/05/09/brumm-mikushin-scheidegger-and-schenk-2015-scalable-high-dimensional-dynamic-stochastic-economic-modeling/</link>
      <pubDate>Mon, 09 May 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/05/09/brumm-mikushin-scheidegger-and-schenk-2015-scalable-high-dimensional-dynamic-stochastic-economic-modeling/</guid>
      <description>&lt;p&gt;An economist, a physicist, and two computer scientists walk into a bar...&lt;/p&gt;
&lt;p&gt;This is a computational paper that describes and algorithm featuring an adaptive sparse grid and discuss implementation details on a sophisticated HPC cluster.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;They provide examples of their algorithm and computation using a standard international real business cycle model. This is not the interesting part of the paper, so I will not focus on it here.&lt;/p&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h3 id=&#34;adaptive-sparse-grids&#34;&gt;Adaptive Sparse Grids&lt;/h3&gt;
&lt;p&gt;The first main contribution of this paper is to introduce economists to a specialized flavor of function approximation.&lt;/p&gt;
&lt;p&gt;The authors use familiar linear Splines; but do so on a sparse, adaptive, and hierarchical grid.&lt;/p&gt;
&lt;p&gt;By sparse I mean that the n-dimensional grid is not composed of the tensor product, or Cartesian product, of all univariate grids. This helps alleviate the curse of dimensionality.&lt;/p&gt;
&lt;p&gt;By adaptive I mean that the knot vector for the grid in each dimension will change as the solution algorithm for the economic problem proceeds. This helps preserve accuracy of function approximation routines when the grid is sparse.&lt;/p&gt;
&lt;p&gt;By hierarchical I mean that the grid for a particular level of refinement is a strict subset of the grid for all higher levels of refinement. This helps reduce computation costs as the basis functions for higher order terms only require a few additional function evaluations instead of a full basis matrix.&lt;/p&gt;
&lt;h3 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h3&gt;
&lt;p&gt;The second main contribution of the paper is an algorithm that leverages this special interpolation scheme to solve high dimensional dynamic stochastic models.&lt;/p&gt;
&lt;p&gt;The main steps of the iterative portion of the algorithm are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with the coarsest refinement level &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;l&lt;/em&gt; = 1&lt;/span&gt; for all dimensions. Call the grid G. Choose a maximum refinement level&lt;/li&gt;
&lt;li&gt;Also Initialize &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;w&lt;/em&gt;&lt;/span&gt; to be the empty set&lt;/li&gt;
&lt;li&gt;While G and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt; are not the same (NOTE: at end of this loop explain the while goes until we are at Lmax or until we don&#39;t add refinement points)
&lt;ul&gt;
&lt;li&gt;For each grid point in &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt; \ &lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt;&lt;/span&gt;
&lt;ul&gt;
&lt;li&gt;Solve the system of non-linear equations characterizing the optimal controls at that grid point (note you will need to interpolate over the current guess of the policy function) to obtain a new guess for the policy rule at that point&lt;/li&gt;
&lt;li&gt;If the distance between the old and new guess for the policy at the grid point is greater than some threshold, add the neighboring points at the next refinement level to G_new&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Set &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt; = &lt;em&gt;G&lt;/em&gt;&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt; = &lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;o&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;em&gt;d&lt;/em&gt; ∪ &lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;w&lt;/em&gt;&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;G&lt;/em&gt;&lt;sub&gt;&lt;em&gt;n&lt;/em&gt;&lt;/sub&gt;&lt;em&gt;e&lt;/em&gt;&lt;em&gt;w&lt;/em&gt; = ∅&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;l&lt;/em&gt; + =1&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Calculate the error for this iteration as the sup norm over the implied policy rule from the current iteration and the previous iteration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that this algorithm will start with very few points across the entire domain and iteratively add points only in regions where the policy rule has dramatic changes. This will naturally cause the grid to adapt and add more grid points in areas of the state space that feature non-linearities or discontinuities.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;This algorithm was implemented by coders who know their stuff! They build on cutting edge software and run their code on state of the art super computers in Switzerland.&lt;/p&gt;
&lt;p&gt;The results are impressive.&lt;/p&gt;
&lt;p&gt;On just one node of the super-computer, they were able to achieve a 30x speedup for their code by utilizing a GPU and multi-threaded parallelism.&lt;/p&gt;
&lt;p&gt;They were then able to scale that code from a single node to up to 2048 nodes to achieve a speedup on the order of 10,000x.&lt;/p&gt;
&lt;p&gt;They compare their algorithm to a non-adaptive sparse grid and find that the log 10 average Euler errors from their algorithm are smaller on average than the sparse grid case, but run time is up 2 to 3 orders of magnitude smaller.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Farias Saure Weintraub (2012) (An approximate dynamic programming approach to solving dynamic oligopoly models)</title>
      <link>http://srg.spencerlyon.com/2016/05/03/farias-saure-weintraub-2012-an-approximate-dynamic-programming-approach-to-solving-dynamic-oligopoly-models/</link>
      <pubDate>Tue, 03 May 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/05/03/farias-saure-weintraub-2012-an-approximate-dynamic-programming-approach-to-solving-dynamic-oligopoly-models/</guid>
      <description>&lt;p&gt;This paper builds on the approximate linear programming work of Farias and van Roy that I presented a few weeks ago and applies a version of that technique to a dynamic oligopoly model.&lt;/p&gt;
&lt;p&gt;The actual model is not novel to this research, so I will spend most of my time talking about the algorithm.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;The model is set in discrete time and multiple firms compete in a single good market over an infinite horizon.&lt;/li&gt;
&lt;li&gt;Firms are identified by a firm specific state x that takes on an integer value between 0 and some upper limit xbar&lt;/li&gt;
&lt;li&gt;The aggregate state s is a histogram of the number of firms at each individual state (a vector of xbar+1 integers)&lt;/li&gt;
&lt;li&gt;The maximum number of incumbent firms is fixed at N. Each period there are &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;N&lt;/em&gt; − &lt;em&gt;s&lt;/em&gt;&lt;em&gt;u&lt;/em&gt;&lt;em&gt;m&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;)&lt;/span&gt; possible entrants. Entrants do not produce or earn profits in the first period.&lt;/li&gt;
&lt;li&gt;Incumbents choose an investment level that determines the probability of remaining in the same state, or moving up or down one step to a neighboring state in the next period.&lt;/li&gt;
&lt;li&gt;Each period the following events occur in this order:
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Incumbent firms draw a random sell-off cost and decide if they want to exit. If they stay, they make investment decisions.&lt;/li&gt;
&lt;li&gt;Each potential entrant draws a random entry cost and makes entry decision&lt;/li&gt;
&lt;li&gt;Incumbent firms compete in spot market and receive prices&lt;/li&gt;
&lt;li&gt;Exiting firms exit and receive sell off values&lt;/li&gt;
&lt;li&gt;Shocks are realized, each firm that stays transitions to a new state.&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The equilibrium concept studied in this paper is a symmetric Markov perfect equilibrium. In this context a MPE is an investment/exit strategy for incumbents and an entry strategy for potential entrants such that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;given that all other incumbents follow the exit/investment strategy, each incumbent does not want to deviate from that strategy&lt;/li&gt;
&lt;li&gt;For all states with a positive number of entrants, the cutoff entry value is equal to the expected discounted value of profits of entering the industry&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;computation&#34;&gt;Computation&lt;/h2&gt;
&lt;h3 id=&#34;naive-algorithm&#34;&gt;Naive algorithm&lt;/h3&gt;
&lt;p&gt;To understand the contribution of this paper, it is helpful to have a basic understanding of a naive &amp;quot;brute force&amp;quot; algorithm.&lt;/p&gt;
&lt;p&gt;The naive algorithm presented here is iterating on a best response operator and proceeds as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Choose some initial investment, exit, and entry policy&lt;/li&gt;
&lt;li&gt;Repeat the following until the policies are close enough:
&lt;ul&gt;
&lt;li&gt;Taking that N-1 players use the current guess for the policy, compute a best response for one agent. To do this you can apply standard dynamic programming algorithms&lt;/li&gt;
&lt;li&gt;Compute some notion of a norm between the best response and the current guess for the policy&lt;/li&gt;
&lt;li&gt;Set the current guess equal to the best response and continue if needed&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This algorithm is robust, but has one major drawback: the curse of dimensionality. For in a model with 20 incumbents and 20 individual states, there are over a thousand billion states that must be iterated over when computing the best response.&lt;/p&gt;
&lt;h3 id=&#34;approximate-dynamic-programming-algorithm&#34;&gt;Approximate dynamic programming algorithm&lt;/h3&gt;
&lt;p&gt;The authors of this paper make 4 modifications to the naive algorithm:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They form the linear program version of the dynamic programming problem to solve for the best response&lt;/li&gt;
&lt;li&gt;They apply approximate linear programming approach of de Farias and van Roy to reduce the number of variables the solver must find&lt;/li&gt;
&lt;li&gt;They use constraint sampling to only enforce a subset of the constraints of the linear program&lt;/li&gt;
&lt;li&gt;They choose basis functions that are especially well suited to their problem&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;The authors do a few numerical experiments. The experiments all revolve around solving for the MPE exactly or approximately and then computing implied long run aggregates, such as average producer and consumer surplus, average share of ith largest firm, and average investment.&lt;/p&gt;
&lt;h4 id=&#34;exercise-1-small-model-exact-comparison&#34;&gt;Exercise 1: small model, exact comparison&lt;/h4&gt;
&lt;p&gt;For a relatively small version of the model (number of incumbents low), they are able to apply the naive algorithm and compute the true MPE. They also solve the model using their proposed algorithm and show that the aggregates are always within 8% of the true values, typically within 2%.&lt;/p&gt;
&lt;p&gt;Runtime for their algorithm is on the order of minutes in each case. With a max of 3 incumbents, runtime for the naive algorithm is on the order of seconds. When the max number of incumbents is 5, the naive algorithm takes a few hours.&lt;/p&gt;
&lt;h4 id=&#34;exercise-2-large-model-compare-oe&#34;&gt;Exercise 2: large model, compare OE&lt;/h4&gt;
&lt;p&gt;They also run a similar experiment for a much larger model where they compare a current state of the art algorithm to their proposed algorithm.&lt;/p&gt;
&lt;p&gt;They find that the aggregates are always within 13% of one another, but often within 5%.&lt;/p&gt;
&lt;p&gt;As the state of the art is also an approximation, this doesn&#39;t say much more than that both algorithms appear to approximate a similar thing.&lt;/p&gt;
&lt;p&gt;They comment that the runtime for a version of the model with 20 incumbents is a couple of hours. Unfortunately they don&#39;t elaborate on the difference in runtime between the algorithms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timoshenko (2015) (Learning versus sunk costs explanations of export persistence)</title>
      <link>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-learning-versus-sunk-costs-explanations-of-export-persistence/</link>
      <pubDate>Tue, 26 Apr 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-learning-versus-sunk-costs-explanations-of-export-persistence/</guid>
      <description>&lt;p&gt;&lt;p&gt;This is a &lt;em&gt;mostly&lt;/em&gt; empirical paper that tries to decompose exporter persistence into sunk cost and learning components.&lt;/p&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;Let firms be indexed by &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt;. Firms have two state variables: productivity &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;, &lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; that follows an AR(1) process and export experience &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;, &lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;. In &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;, &lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; is the number of consecutive periods a firm has exported coming into period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Without microfoudations, Timoshenko assumes that per-period sales are the Melitz result, multiplied by a function &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; of export experience:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math inline&#34;&gt;$S(z, A) = g(A) E P^{\sigma-1} \left( \frac{\sigma}{\sigma-1} \frac{\tau w}{\exp(z)} \right)^{1-\sigma}$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If a firm chooses to export, profits are given by &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;π&lt;/em&gt;(&lt;em&gt;z&lt;/em&gt;, &lt;em&gt;A&lt;/em&gt;)=&lt;em&gt;S&lt;/em&gt;(&lt;em&gt;z&lt;/em&gt;, &lt;em&gt;A&lt;/em&gt;)/&lt;em&gt;σ&lt;/em&gt;&lt;/span&gt; less a sunk market-entry cost if &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt; = 0&lt;/span&gt;. If a firm does not export, period profits are 0.&lt;/p&gt;
&lt;p&gt;Timoshenko assumes that &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt; is increasing in &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt;. She calls this the age-dependence (or learning) assumption and uses it to get empirical results later on.&lt;/p&gt;
&lt;p&gt;Export decision problem has a policy rule characterized by a pair of cutoff productivities &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt; &amp;gt; &lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;: a exit threshold &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt; and an entry threshold &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;. Firms enter when they get a productivity draw above &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;. They continue to export until productivity falls below &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;l&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;A&lt;/em&gt;)&lt;/span&gt; is a function of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt; because there is a hysteresis effect caused by &lt;span class=&#34;math inline&#34;&gt;$\frac{\partial g(A)}{\partial A} &amp;gt; 0$&lt;/span&gt;. &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;z&lt;/em&gt;&lt;sub&gt;&lt;em&gt;h&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; is not a function of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt;, because by definition of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt; = 0&lt;/span&gt; for all firms considering entry.&lt;/p&gt;
&lt;h2 id=&#34;empirics&#34;&gt;Empirics&lt;/h2&gt;
&lt;p&gt;Timoshenko then does some empirical work to try to tease out the importance of the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; function on export sales and participation. Because there are sunk export costs in the model, she claims that considering both &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; and sunk costs isolates all learning effects into coefficients for &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;To do this she starts from her reduced form theoretical model to build an empirical model. She controls for firm characteristics like location, domestic sales, investment, labor and materials expenditures, capital stock, wage rate, productivity, etc. She also builds in plant specific random effects. The result is an system of estimable equations that effectively regress the export decision rule on firm characteristics, exporter age dummies, and other controls.&lt;/p&gt;
&lt;h3 id=&#34;aside&#34;&gt;Aside&lt;/h3&gt;
&lt;p&gt;I don&amp;rsquo;t necessarily agree with her main claim. Without micro-foundations, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; doesn&amp;rsquo;t say anything about learning specifically. As it is written &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;g&lt;/em&gt;&lt;/span&gt; is a simply a mechanism that causes sales and profits to increase with exporter age. This could be learning, more efficient production, more efficient transporting of goods, exogenous rise in foreign demand, market penetration/firm reputation in foreign market, etc. You might make an argument that the efficiency angles are a subset of learning: firms are &lt;em&gt;learning&lt;/em&gt; to be more efficient, but I believe separating different channels of learning is important.&lt;/p&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;Using plant level data on Columbian firms in the 1980&amp;rsquo;s Timoshenko finds evidence that supports her age-dependence hypothesis. The findings are robust across industries. The punchline is that about &amp;frac12; of the value of the state dependence parameter is attributed to sunk costs and about &amp;frac12; is attributed to age-dependence. By state dependence I mean the impact of yesterday&amp;rsquo;s export status on today&amp;rsquo;s export decision.&lt;/p&gt;
&lt;div id=&#34;refs&#34; class=&#34;references&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;/div&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Timoshenko (2015) (Product switching in a model of learning)</title>
      <link>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-product-switching-in-a-model-of-learning/</link>
      <pubDate>Tue, 26 Apr 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/04/26/timoshenko-2015-product-switching-in-a-model-of-learning/</guid>
      <description>&lt;p&gt;This paper documents facts about Brazilian manufacturing firms that switch their bundle of exported products over time. The author then builds a Melitz-style trade model that attempts to explain these facts.&lt;/p&gt;
&lt;h2 id=&#34;empirics&#34;&gt;Empirics&lt;/h2&gt;
&lt;p&gt;The author uses data product level data on Brazilian manufacturing firms to document several stylized facts.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;72% of continuing exporters alter their product mix every year (add new exported products or drop existing exported products)&lt;/li&gt;
&lt;li&gt;83% of all Brazilian exports come from these product switching firms&lt;/li&gt;
&lt;li&gt;The proportion of exporters who do product switching falls with age in export market&lt;/li&gt;
&lt;li&gt;The frequency with which exporters engage in product switching falls with age&lt;/li&gt;
&lt;li&gt;The exit rate of product switching firms is lower than aggregate exit rate for all ages of exporter.&lt;/li&gt;
&lt;li&gt;Conditional on an exporter adding new products, over 1/2 of exporter products are added products&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;model&#34;&gt;Model&lt;/h2&gt;
&lt;p&gt;The model in this paper extends Melitz (2003) in two ways:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Consumers have firm specific demand shocks in their consumption aggregator&lt;/li&gt;
&lt;li&gt;Firms of a particular brand produce a finite, discrete number of products&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here&#39;s the main points of the model:&lt;/p&gt;
&lt;h3 id=&#34;consumers&#34;&gt;Consumers&lt;/h3&gt;
&lt;p&gt;In each of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;N&lt;/em&gt;&lt;/span&gt; countries, there is a representative consumer that has log preferences over an aggregate consumption good.&lt;/p&gt;
&lt;p&gt;The aggregate good is a CES aggregated bundle of firm goods from all &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;N&lt;/em&gt;&lt;/span&gt; countries. Each firm good is hit with a time-varying, firm-and-import-country-specific demand shock.&lt;/p&gt;
&lt;p&gt;The firm goods are a CES aggregation of a finite number of differentiated products produced by the firm.&lt;/p&gt;
&lt;p&gt;Consumers inelastically supply a country specific fixed labor each period and earn labor income from firms. They also own firms in their country and retain all profits.&lt;/p&gt;
&lt;p&gt;Consumers take prices and wages as given maximize the utility of consuming the aggregate good subject to total expenditures being equal to labor income plus firm profits.&lt;/p&gt;
&lt;p&gt;The output of the consumer problem is a demand function for each product from each firm in each country.&lt;/p&gt;
&lt;p&gt;This demand function is a function of the consumer&#39;s income, the current demand shock for the firm and prices. Crucially, it can be inverted to give prices as a function of quantity and the demand shock.&lt;/p&gt;
&lt;h3 id=&#34;firms&#34;&gt;Firms&lt;/h3&gt;
&lt;p&gt;Firm&#39;s differ in their ability &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;φ&lt;/em&gt;&lt;/span&gt; to produce all their products. This ability or productivity is constant over time and across products and is drawn from a Pareto distribution.&lt;/p&gt;
&lt;p&gt;The demand shock in the consumer&#39;s CES aggregator is the sum of a constant firm specific component and a mean zero normally distributed iid shock drawn each period. The firm specific constant component is drawn from a normal distribution with known (to the firm) mean and variance. The prior beliefs of every firm are that the firm specific demand shock is drawn from its true distribution. Because signals and priors are normal, posterior beliefs are also normal and sufficient statistics are the mean and variance of the firm specific component.&lt;/p&gt;
&lt;p&gt;Firms choose the quantity of each product to be sold in each country, &lt;em&gt;before&lt;/em&gt; seeing the value of the demand shock for their firm in a given period. Consumers&#39; see this quantity and use their optimality conditions to declare the price they are willing to pay for the good. The firm observes this prices, which can be manipulated to reveal the current level of the demand shock. This acts as a signal about the firm specific constant component of the demand shock. Firms are Bayesian and update their beliefs about this component each period after observing the demand shock.&lt;/p&gt;
&lt;p&gt;Firms face fixed per-period cost of selling into each country. These costs increase with the number of products produced.&lt;/p&gt;
&lt;p&gt;Conditional on entering into a specific market, each period firms choose the number of products to be sold into the market as well as a quantity of each of these goods to maximize the expected value of their one period profits. Firm&#39;s operate in monopolistically competitive markets, so they understand how their quantity decision will impact the market clearing price. However, they do not know the current value of the demand shock, so they form expectations of the price using their current beliefs about the firm specific demand shock. The output of this problem is a quantity for each product as well as an expected profit for each profit.&lt;/p&gt;
&lt;p&gt;Firms continue to add products until the expected profits from adding more varieties (taking into account the fixed costs) are negative.&lt;/p&gt;
&lt;p&gt;Finally, firms face an entry and exit decision. A firm with productivity &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;φ&lt;/em&gt;&lt;/span&gt; has as state variables the sufficient statistics for their beliefs regarding the firm specific demand shock. They then choose whether or not to enter each market using the expected profit functions we just discussed.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Timoshenko looks at a symmetric, stationary equilibrium of this economy. Some properties of this equilibrium are are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The market participation policy is given by a cutoff threshold for the current expected value of the firm specific demand shock in terms of productive and other sufficient statistics. This cutoff is decreasing in productivity and increasing in the precision (inverse of variance).&lt;/li&gt;
&lt;li&gt;The quantity adjustment in response to seeing another signal about the demand shock is positive when the posterior mean is sufficiently high and negative when it is lower. The cutoff is a function of the prior mean and variance and the variance of the signal.&lt;/li&gt;
&lt;li&gt;Profits scale with quantities, so many high signals expands quantities and profits, and causes firms to add new products. A sequence of low signals has the opposite effect: lower quantities and profits leading to dropping products.&lt;/li&gt;
&lt;li&gt;Firms posterior precision increases deterministically, meaning asymptotically firms perfectly learn the firm specific component of the demand shocks. This generates age effects in quantity decisions and profits, which in turn generate age effects in adding and dropping products that match the data.&lt;/li&gt;
&lt;li&gt;If trade costs are lowered, the quantity of all current products is expanded. This is &lt;em&gt;not&lt;/em&gt; supported empirically. In the data, lower trade costs tend to have firms specialize more -- meaning they increase qualities of their most profitable products and scale back on quantities of marginal products.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;quantitative-results&#34;&gt;Quantitative results&lt;/h3&gt;
&lt;p&gt;A brief quantitative section is given. The main message is that the model generates more modest values of the number of exporters that engage in product switching, the age dependent survival rate of exporters, and the age dependence of product switching. This suggests that the learning mechanism in the model is significant and supported by the data, but not sufficient to fully explain product switching behavior of Brazilian firms.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Konda and Tsitsiklis (2003) (Actor-critic Algorithms)</title>
      <link>http://srg.spencerlyon.com/2016/04/19/konda-and-tsitsiklis-2003-actor-critic-algorithms/</link>
      <pubDate>Tue, 19 Apr 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/04/19/konda-and-tsitsiklis-2003-actor-critic-algorithms/</guid>
      <description>&lt;p&gt;&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;When a Markov decision process (MDP) is formulated as a dynamic programming problem, the reinforcement learning literature proposes are two classic classes of algorithms to solve them. Let&amp;rsquo;s briefly review these types of algorithms and point out strengths and weaknesses of each.&lt;/p&gt;
&lt;h4 id=&#34;actor-only-methods&#34;&gt;1: Actor only methods&lt;/h4&gt;
&lt;p&gt;We can think of an actor as a fictitious character that operates on a policy rule.&lt;/p&gt;
&lt;p&gt;When I talk about the performance of an actor, I mean the value of following a policy.&lt;/p&gt;
&lt;p&gt;These methods are often implemented by estimating the gradient of the performance of an actor using simulation.&lt;/p&gt;
&lt;p&gt;There are two main issues with these &amp;quot;policy iteration&amp;quot;-esque algorithms:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Gradient estimators can have high variance&lt;/li&gt;
&lt;li&gt;As the policy changes, a new gradient is estimated independently. This means there is no sense of learning from past data.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id=&#34;critic-only-methods&#34;&gt;2: Critic only methods&lt;/h4&gt;
&lt;p&gt;We can think of a critic operating on either a Q or V value function.&lt;/p&gt;
&lt;p&gt;These methods rely exclusively on value function approximation and try to learn the solution to Bellman&amp;rsquo;s equation.&lt;/p&gt;
&lt;p&gt;The main issues with critic only methods are:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;They are indirect in that they do not try to optimize directly over the policy space&lt;/li&gt;
&lt;li&gt;Even with an accurate approximation of the value function, results that guarantee the near-optimality of the corresponding policy are difficult to guarantee.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;main-idea&#34;&gt;Main idea&lt;/h2&gt;
&lt;p&gt;This paper suggests two actor-critic hybrid methods that aim to maintain the best features of each algorithm, while overcoming the shortcomings mentioned above.&lt;/p&gt;
&lt;p&gt;The main idea behind the algorithms is that the critic uses a linearly parameterized approximation scheme and simulation to learn a value function. The actor then uses the learned information to update parameters on the policy function in a direction of performance improvement.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Aside to tie back to econ&lt;/em&gt;: This feels like modified policy iteration or Howard&amp;rsquo;s improvement algorithm, but it is different in a few ways:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;There is a learning element to these algorithms, which means we don&amp;rsquo;t have to compute expectations explicitly.&lt;/li&gt;
&lt;li&gt;We will be learning Q functions, which describe the value of being in a state and taking any feasible action (instead of the V function that describes the value of being in a state and choosing the optimal action).&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;algorithms&#34;&gt;Algorithms&lt;/h2&gt;
&lt;!-- The algorithms are presented in terms of identifying a randomized stationary
policy. We typically think of a policy function as assigning a unique action to
each state. A randomized stationary policy attaches a probability distribution
over actions to each state. In some sense a standard policy is like a pure
strategy, whereas a randomized stationary is like a mixed strategy. --&gt;
&lt;p&gt;The presentation is very technical and relies on assumptions that aren&amp;rsquo;t necessarily applicable to the models we write down, so I won&amp;rsquo;t the paper exactly as it was written. Instead, I will sketch the algorithm and explain the key insight the authors have that makes the algorithm tractable.&lt;/p&gt;
&lt;h3 id=&#34;setup&#34;&gt;Setup&lt;/h3&gt;
&lt;p&gt;We will represent the critic using three variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A coefficient vector of length &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;m&lt;/em&gt;&lt;/span&gt; that describe a linear parametrization of Q in terms of basis functions.&lt;/li&gt;
&lt;li&gt;A scalar &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;α&lt;/em&gt;&lt;/span&gt; that represents the average value of following the actor&amp;rsquo;s policy&lt;/li&gt;
&lt;li&gt;A vector of length &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;m&lt;/em&gt;&lt;/span&gt; that represents Sutton&amp;rsquo;s eligibility trace. This vector is used to form a bridge between fixed point methods and Monte Carlo methods&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The actor is represented by a suitable parametric representation of the policy function.&lt;/p&gt;
&lt;h3 id=&#34;algo&#34;&gt;Algo&lt;/h3&gt;
&lt;p&gt;In order to understand the algorithm, I need to provide two related definitions. A &lt;strong&gt;temporal difference&lt;/strong&gt; is the difference between the current approximation of a variable and a realization of that variable. In other words it is the error in our approximation for a particular sample.&lt;/p&gt;
&lt;p&gt;We say we &lt;em&gt;update&lt;/em&gt; parameters or approximations using a &lt;strong&gt;temporal difference&lt;/strong&gt; if the new approximation is the sum of the current approximation and a scaled temporal difference.&lt;/p&gt;
&lt;p&gt;The algorithm &lt;em&gt;roughly&lt;/em&gt; proceeds as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize the actor and critic&lt;/li&gt;
&lt;li&gt;Perform one step updates of the actor and critic as follows:
&lt;ul&gt;
&lt;li&gt;Because we learn Q, we enter a time period with a particular state and action in hand&lt;/li&gt;
&lt;li&gt;The actor will dictate a new action and we need to simulate a new state, potentially given that action&lt;/li&gt;
&lt;li&gt;The critic&amp;rsquo;s parameters are updated according to:
&lt;ul&gt;
&lt;li&gt;Average value of policy: temporal difference update (using flow implied by state and action)&lt;/li&gt;
&lt;li&gt;Coefficient vector for Q: temporal difference update, scaled by eligibility trace&lt;/li&gt;
&lt;li&gt;Eligibility trace:&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;The actor&amp;rsquo;s parameter vector is updated using a gradient approach that resembles Newton&amp;rsquo;s method. It takes into account updates to the actor&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key insight the authors have that make this algorithm tractable is the following:&lt;/p&gt;
&lt;p&gt;Actors have to update a small number of parameters compared to the number of states. So the critic doesn&amp;rsquo;t need to form an approximation over the entire domain of Q, but rather a special projection of the Q onto the space spanned by the actor&amp;rsquo;s parameter vector.&lt;/p&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Busoniu, DeSchutter, Babuska(2010) (Approximate dynamic programming and reinforcement learning)</title>
      <link>http://srg.spencerlyon.com/2016/04/12/busoniu-deschutter-babuska2010-approximate-dynamic-programming-and-reinforcement-learning/</link>
      <pubDate>Tue, 12 Apr 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/04/12/busoniu-deschutter-babuska2010-approximate-dynamic-programming-and-reinforcement-learning/</guid>
      <description>&lt;p&gt;This paper provides an overview of various solution concepts that belong to the families of approximate dynamic programming and reinforcement learning.&lt;/p&gt;
&lt;p&gt;I will discuss an alternative representation of a dynamic programming that is often used in this literature and provide an overview of some solution methods that are common with this representation.&lt;/p&gt;
&lt;h2 id=&#34;basics-q-functions&#34;&gt;Basics: Q-functions&lt;/h2&gt;
&lt;p&gt;In economics, the standard representation of a dynamic programming problem is to define a recursive function &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt; that maps from a state space into the real line. Today I will call &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt; the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt; function. The &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt; function describes the value of being in a particular state.&lt;/p&gt;
&lt;p&gt;Consider an alternative function &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; that maps from the state-action space, into the real line. I will call this the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;-function. The &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; function describes the value of being in a particular state and choosing a particular action.&lt;/p&gt;
&lt;p&gt;As we do with the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt;-function, we can express the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;-function recursively using Bellman&#39;s principle of optimality. The recursive form of the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;-function is&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;Q&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;, &lt;em&gt;x&lt;/em&gt;)=&lt;em&gt;E&lt;/em&gt;&lt;sub&gt;&lt;em&gt;s&lt;/em&gt;′∼&lt;em&gt;f&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;, &lt;em&gt;x&lt;/em&gt;, ⋅)&lt;/sub&gt;[&lt;em&gt;r&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;,&lt;em&gt;x&lt;/em&gt;,&lt;em&gt;s&lt;/em&gt;′)+&lt;em&gt;β&lt;/em&gt;max&lt;sub&gt;&lt;em&gt;x&lt;/em&gt;′&lt;/sub&gt;&lt;em&gt;Q&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;′,&lt;em&gt;x&lt;/em&gt;′)]&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;We can make some basic comparisons between &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt; by studying their recursive representations:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parameteric representations (i.e. splines or other interpolands) of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; are more expensive than representations of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt; because &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; is defined over the state-action space instead of just the state space.&lt;/li&gt;
&lt;li&gt;Computing the maximization on the right hand side of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; is easier than the max on the RHS of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt;. This is because the expectation is &lt;em&gt;outside&lt;/em&gt; the max in &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;, but inside the max for &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The Bellman operator associated with the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; function is a contraction mapping under the same conditions that Bellman operator associated &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt; is a contraction mapping. When &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;&#39;s operator is a contraction mapping, the operators&#39; unique fixed point &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt; is the optimal &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; function.&lt;/p&gt;
&lt;p&gt;The optimal policy can be easily computed from &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;$h^* (s) = \underset{x}{\text{argmax}} Q^* (s, x)$&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The optimal &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;/span&gt;-function can be obtained from &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;: &lt;span class=&#34;math inline&#34;&gt;$V^* = \underset{x}{\max} Q^* (s, x)$&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;computing-q&#34;&gt;Computing Q&lt;/h2&gt;
&lt;p&gt;We see that if we can find &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;, we can get the value and policy functions we care about as economists. I want to briefly describe a few flavors of algorithms that can be used to compute &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;q-learning&#34;&gt;Q-learning&lt;/h3&gt;
&lt;p&gt;Q-learning is classified as an online, model-free algorithm. This means you can update your guess for &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt; whenever new data becomes available. Economists would classify this as a simulation algorithm.&lt;/p&gt;
&lt;p&gt;Here&#39;s the basics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with any initial guess for the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; function&lt;/li&gt;
&lt;li&gt;Then on each iteration &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt;:
&lt;ul&gt;
&lt;li&gt;Given the state action pair on the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt;th iteration (&lt;span class=&#34;math inline&#34;&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;), the observation of the period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt; return &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;r&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;, and the state iteration &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/sub&gt;&lt;/span&gt;...&lt;/li&gt;
&lt;li&gt;... update your guess for &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; using: &lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/sub&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)=(1 − &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;em&gt;Q&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)+&lt;em&gt;α&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;r&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;+&lt;em&gt;β&lt;/em&gt;max&lt;sub&gt;&lt;em&gt;x&lt;/em&gt;′&lt;/sub&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/sub&gt;,&lt;em&gt;u&lt;/em&gt;′))&lt;/span&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Notice that the right hand side is a convex combination of the current guess of the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt; function and the right hand side of the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;-function operator. This is very similar to standard value function iteration, but has the main advantage of not having any expectations.&lt;/p&gt;
&lt;p&gt;This can be applied to any model for which you have a transition function that defines the state today as a function of the state and action yesterday and a random innovation from a distribution you can sample. Many economic models fit into this framework.&lt;/p&gt;
&lt;p&gt;When the action space is continuous, you need do a modest alteration of the basic algorithm to ensure asymptotic convergence to &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h3 id=&#34;approximate-q-iteration&#34;&gt;Approximate Q-iteration&lt;/h3&gt;
&lt;p&gt;One strength of this literature is the number of different ways of reducing the dimensionality of a dynamic programming problem.&lt;/p&gt;
&lt;p&gt;Approximate Q-iteration is one example of this type of algorithm. Here instead of operating on the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;-function directly, we instead operate on a parameterized approximation of the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Q&lt;/em&gt;&lt;/span&gt;-function.&lt;/p&gt;
&lt;p&gt;This can be done using linear dependence on coefficients (splines, Chebyshev polynomials) or non-linear parametric approximations (neural nets).&lt;/p&gt;
&lt;p&gt;We&#39;ll focus on the linearly parametrized version here. We will form an approximation &lt;span class=&#34;math inline&#34;&gt;$\hat{Q}(s, x) = \sum_{l=1}^n \phi_l(s, x) \theta_l = \Phi\theta$&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We and talk about a variant of the Q-learning we previously discussed that uses gradient information to update the coefficient vector.&lt;/p&gt;
&lt;p&gt;Here&#39;s the basic idea:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with any guess of coefficients &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Then on each iteration &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;k&lt;/em&gt;&lt;/span&gt;:
&lt;ul&gt;
&lt;li&gt;Given the state action pair on the &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt;th iteration (&lt;span class=&#34;math inline&#34;&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;), the observation of the period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt;&lt;/span&gt; return &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;r&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;, and the state iteration &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/span&gt; &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/sub&gt;&lt;/span&gt;...&lt;/li&gt;
&lt;li&gt;... update your guess for &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;θ&lt;/em&gt;&lt;/span&gt; using: &lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;$$\theta_{i+1} = \theta_i + \alpha_i \left[r_i + \beta \max_{x&#39;} \hat{Q}_i(s_{i+1}, x&#39;)  - \hat{Q}_i(s_i, x_i)\right] \frac{\partial}{\partial \theta_i} \hat{Q}_i(s_i, x_i)$$&lt;/span&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;In the linear parameterization world we are in this becomes &lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/sub&gt; = &lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; + &lt;em&gt;α&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;[&lt;em&gt;r&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;+&lt;em&gt;β&lt;/em&gt;max&lt;sub&gt;&lt;em&gt;x&lt;/em&gt;′&lt;/sub&gt;(&lt;em&gt;Φ&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt; + 1&lt;/sub&gt;,&lt;em&gt;x&lt;/em&gt;′)&lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)−&lt;em&gt;Φ&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;,&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;em&gt;θ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;]&lt;em&gt;Φ&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt;&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As this is a Q-learning algorithm, the same computational benefits arise here, but now we additionally have the benefit of a potentially vastly reduced computational problem depending on the state-action space the choice of basis functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>de Farias &amp; van Roy (2003) (The Linear Programming Approach to Approximate Dynamic Programming)</title>
      <link>http://srg.spencerlyon.com/2016/03/29/de-farias--van-roy-2003-the-linear-programming-approach-to-approximate-dynamic-programming/</link>
      <pubDate>Tue, 29 Mar 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/03/29/de-farias--van-roy-2003-the-linear-programming-approach-to-approximate-dynamic-programming/</guid>
      <description>&lt;p&gt;This paper was published in Operations Research and as such they use a different notation and jargon than economists. I&#39;ll present some of their main results, but in a language and notation that is familiar to me.&lt;/p&gt;
&lt;h2 id=&#34;background&#34;&gt;Background&lt;/h2&gt;
&lt;p&gt;Consider a typical dynamic programming problem faced by an economic agent, which we summarize by the following Bellman equation:&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;$$V(x) = \underset{c \in \Gamma(x)}{\max} u(x, c) + \beta E V(x&#39;)$$&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Now define the natural operator associated with the Bellman, which we call &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;$$T v = \underset{c \in \Gamma(x)}{\max} u(x, c) + \beta E v(x&#39;)$$&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;For ease of presentation we will directly assume that &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt; is a contraction mapping and that &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt; belongs to the set of bounded and continuous functions.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt; be the optimal value function. Under our assumptions &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt; is the unique fixed point of Bellman&#39;s equation.&lt;/p&gt;
&lt;h2 id=&#34;mathematical-programs&#34;&gt;Mathematical programs&lt;/h2&gt;
&lt;p&gt;Under our assumptions, the operator &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt; is monotonic.&lt;/p&gt;
&lt;p&gt;This means that for any &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt; such that &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt; ≥ &lt;em&gt;T&lt;/em&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt; we have &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt; ≥ &lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Also &lt;span class=&#34;math inline&#34;&gt;∀&lt;em&gt;v&lt;/em&gt;&lt;em&gt;s&lt;/em&gt;&lt;em&gt;t&lt;/em&gt;&lt;em&gt;v&lt;/em&gt; ≤ &lt;em&gt;T&lt;/em&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt; ≤ &lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Typically this property of our contraction mapping is used to show that value function iteration will converge for any initial guess of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt;. Today we will use it in a slightly different way.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;c&lt;/em&gt;&lt;/span&gt; be any vector of all positive elements. Consider the mathematical program defined by&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;$$\begin{aligned}&amp;amp;\underset{V}{\min} c&#39;v \\
s.t. &amp;amp; \quad v \ge T v
\end{aligned}$$&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;By the monotonicity of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt; it is easy to see that any &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt; satisfying the constraint must be at least as big at &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Then notice that the objective is to minimize the inner product between a strictly positive vector &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;c&lt;/em&gt;&lt;/span&gt; and the choice &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;These two facts mean that the unique solution to the programming problem is &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;sup&gt;*&lt;/sup&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;linear-program&#34;&gt;Linear program&lt;/h2&gt;
&lt;p&gt;I told you that we would use linear programming, but notice that the constraint is nonlinear because &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt; is nonlinear.&lt;/p&gt;
&lt;p&gt;However, if we stare at the definition of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt; for long enough we will notice that if we consider the constraint &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt; ≥ &lt;em&gt;T&lt;/em&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt; state by state, (e.g. &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;)≥&lt;em&gt;T&lt;/em&gt;&lt;em&gt;v&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;)&lt;/span&gt;) we will notice that we can replace the single non-linear constraint per state with a system of linear constraints for each state.&lt;/p&gt;
&lt;p&gt;This system is defined by enumerating all feasible actions for each state and writing down the right hand side of Bellman&#39;s equation for that state and control. The program looks as follows:&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;$$\begin{aligned}&amp;amp;\underset{V}{\min} c&#39;v \\
s.t. &amp;amp; \quad v(x) \ge u(x,c) + \beta \sum_{x&#39;} P(x&#39;| x,c)v(x&#39;) \quad \forall x \in X \; \forall c \in \mathcal{A}_x
\end{aligned}$$&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h2 id=&#34;curse-of-dimensionality&#34;&gt;Curse of dimensionality&lt;/h2&gt;
&lt;p&gt;This linear program has an &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;S&lt;/em&gt;&lt;/span&gt; dimensional state with an &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;S&lt;/em&gt; * &lt;em&gt;A&lt;/em&gt;&lt;/span&gt; dimensional constraint matrix. When &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;S&lt;/em&gt;&lt;/span&gt; or &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;A&lt;/em&gt;&lt;/span&gt; are large, this problem can quickly become subject to the curse of dimensionality.&lt;/p&gt;
&lt;p&gt;The authors of this paper propose approximate linear programming as a way to resolve this issue.&lt;/p&gt;
&lt;p&gt;Specifically they choose to represent &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;v&lt;/em&gt;&lt;/span&gt; as the product of a basis matrix and a vector of coefficients &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;r&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;They then write down a linear program&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;$$\begin{aligned}&amp;amp;\underset{V}{\min} c&#39;\Phi r \\
s.t. &amp;amp; \quad \Phi r(x) \ge u(x,c) + \beta \sum_{x&#39;} P(x&#39;| x,c) \Phi r(x&#39;) \quad \forall x \in X \; \forall c \in \mathcal{A}_x
\end{aligned}$$&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Notice now that we have swapped out a vector of length &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;S&lt;/em&gt;&lt;/span&gt;, for a vector with length equal to the number of columns in the basis matrix. This is something that we have control over, thus we can choose the &amp;quot;size&amp;quot; of this problem. Thus the objective is smaller, but the number of constraints is exactly the same.&lt;/p&gt;
&lt;p&gt;However, if we choose each column of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;Φ&lt;/em&gt;&lt;/span&gt; to have finite support (i.e. we use splines), most constraints become inactive and the large constraint matrix becomes sparse.&lt;/p&gt;
&lt;p&gt;The remainder of the paper, and its main contribution, is to bound the error we are subject to by solving the approximate linear program instead of the exact linear program.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Hull (2015) (Approximate dynamic programming with post-decision states as a solution method for dynamic economic models)</title>
      <link>http://srg.spencerlyon.com/2016/03/22/hull-2015-approximate-dynamic-programming-with-post-decision-states-as-a-solution-method-for-dynamic-economic-models/</link>
      <pubDate>Tue, 22 Mar 2016 00:00:00 +0000</pubDate>
      <author>spencer.lyon@stern.nyu.edu (Spencer Lyon)</author>
      <guid>http://srg.spencerlyon.com/2016/03/22/hull-2015-approximate-dynamic-programming-with-post-decision-states-as-a-solution-method-for-dynamic-economic-models/</guid>
      <description>&lt;p&gt;This paper presents a stochastic simulation method for solving dynamic economic models.&lt;/p&gt;
&lt;p&gt;The ideas in this paper lean on a literature sometimes known as approximate dynamic programming and can enable us to solve models with many state variables and non-convexities in objectives and constraints.&lt;/p&gt;
&lt;p&gt;My intent is to summarize the core theoretical ideas behind the algorithm.&lt;/p&gt;
&lt;h2 id=&#34;main-idea-post-decision-states&#34;&gt;Main idea: Post decision states&lt;/h2&gt;
&lt;h3 id=&#34;notation-classic-bellman-equation&#34;&gt;Notation: classic Bellman equation&lt;/h3&gt;
&lt;p&gt;Consider a stationary economic model where at time &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt; the state is summarized by a vector &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; of endogenous state variables and a vector &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; of exogenous state variables.&lt;/p&gt;
&lt;p&gt;The optimization problem of an agent is often summarized by a Bellman equation of the form&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;V&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;)=&lt;em&gt;m&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;em&gt;u&lt;/em&gt;(&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;)+&lt;em&gt;β&lt;/em&gt;&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;V&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt;)]&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;subject to&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt; = &lt;em&gt;f&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;), &lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt; ∈ &lt;em&gt;Γ&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;), &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt; = &lt;em&gt;g&lt;/em&gt;(&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;ϵ&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt;)&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&#34;post-decision-state-value-function&#34;&gt;Post-decision state value function&lt;/h3&gt;
&lt;p&gt;Note that that the transition function for the endogenous state takes &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;, and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; and emits &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt;&lt;/span&gt;. We can think of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt;&lt;/span&gt; being chosen at the end of period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;, meaning after the controls &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; have been decided.&lt;/p&gt;
&lt;p&gt;In the standard (or pre-decision) Bellman equation, we have that the state at time &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt; is the endogenous state defined at the end of period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/span&gt; and the exogenous state realized at the start of period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We will now consider a different representation of the state vector that couples the endogenous state defined at the end of period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/span&gt; with the exogenous state realized at the start of period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/span&gt;. That is we will consider &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;&lt;/span&gt;, which is known as the post-decision state at time &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;)&lt;/span&gt; be the value of having post-decision state &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt;, &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;&lt;/span&gt; in period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/span&gt;. This is the maximum expected, discounted utility an agent can achieve after controls have been selected in period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Because &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;&lt;/span&gt; is not chosen until &lt;em&gt;after&lt;/em&gt; &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;&lt;/span&gt; is realized, we know that &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;)&lt;/span&gt; is equal to the expectation of the maximum expected, discounted utility the agent will receive after &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; arrives. That is, we can write&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;)=&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;V&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;)|&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;],&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;)&lt;/span&gt; is the pre-decision Bellman equation.&lt;/p&gt;
&lt;p&gt;It follows that we can write&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;V&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;)=&lt;em&gt;m&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/sub&gt; + &lt;em&gt;β&lt;/em&gt;&lt;em&gt;V&lt;/em&gt;&lt;sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;).&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;These equations can be manipulated to produce the recursive form of the post-decision state Bellman equation:&lt;/p&gt;
&lt;p&gt;&lt;br /&gt;&lt;span class=&#34;math display&#34;&gt;&lt;em&gt;V&lt;/em&gt;&lt;sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;)=&lt;em&gt;E&lt;/em&gt;[&lt;em&gt;m&lt;/em&gt;&lt;em&gt;a&lt;/em&gt;&lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;em&gt;u&lt;/em&gt;(&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;)+&lt;em&gt;β&lt;/em&gt;&lt;em&gt;V&lt;/em&gt;&lt;sup&gt;&lt;em&gt;x&lt;/em&gt;&lt;/sup&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; + 1&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;)|&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt; − 1&lt;/sub&gt;].&lt;/span&gt;&lt;br /&gt;&lt;/p&gt;
&lt;p&gt;Notice that the expectation is &lt;em&gt;outside&lt;/em&gt; the max operator, meaning that the maximization problem is deterministic.&lt;/p&gt;
&lt;h2 id=&#34;algorithm&#34;&gt;Algorithm&lt;/h2&gt;
&lt;p&gt;Now that we have the post-decision state Bellman equation, the algorithm is fairly straightforward. I will present the algorithm from the paper in the context of Markov exogenous processes, but I believe it is incorrectly specified. I&#39;ll discuss how I&#39;d change it later.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Setup
&lt;ul&gt;
&lt;li&gt;Discretize endogenous state space&lt;/li&gt;
&lt;li&gt;Choose a simulation length &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;T&lt;/em&gt;&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;Choose initial endogenous and exogenous states&lt;/li&gt;
&lt;li&gt;Construct an initial guess for the value function at the discritized endogenous and exogenous states.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Iterations
&lt;ul&gt;
&lt;li&gt;Construct a time series of Exogenous states for t=1, 2, ..., T&lt;/li&gt;
&lt;li&gt;For time &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;t&lt;/em&gt; = 1, 2, ..., &lt;em&gt;T&lt;/em&gt;&lt;/span&gt; perform the following 3 steps:
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Choose controls &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;c&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;&lt;/span&gt; to maximize the term inside the expectation on the RHS of &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;V&lt;/em&gt;(&lt;em&gt;s&lt;/em&gt;&lt;sub&gt;&lt;em&gt;t&lt;/em&gt;&lt;/sub&gt;, &lt;em&gt;x&lt;/em&gt;&lt;em&gt;t&lt;/em&gt; − 1)&lt;/span&gt;. To do this we need to using the value function from the previous iteration for the future value function&lt;/li&gt;
&lt;li&gt;Compute the expectation implicitly by updating the guess of the value function using a convex combination of the previous iteration&#39;s value function and the value computed above&lt;/li&gt;
&lt;li&gt;Using the chosen controls and realization of exogenous state, apply the endogenous transition equation to iterate the endogenous state forward one period&lt;/li&gt;
&lt;/ol&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Convergence:
&lt;ul&gt;
&lt;li&gt;Check a convergence criterion that compares the discretized value function across multiple iterations.&lt;/li&gt;
&lt;li&gt;If converged, return the discretized value function and run a regression on the time series to obtain a policy function from the time series of controls&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;comments-on-the-algorithm&#34;&gt;Comments on the algorithm&lt;/h3&gt;
&lt;p&gt;Here are a few comments about the algorithm:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Because the expectation operator is outside the max operator, we don&#39;t have to spend time computing expectations when solving the optimization problem in each period of the simulation. This speeds up computation quite a bit.&lt;/li&gt;
&lt;li&gt;Expectations are computed implicitly when we update our guess for the post-decision state value function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;My compliant&lt;/strong&gt; I think it is incorrect to re-generate a time series of exogenous states on each iteration. Doing so will not allow the algorithm to ever converge as the updated value function in iteration &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;n&lt;/em&gt;&lt;/span&gt; is dependent on the randomness from the exogenous simulation in period &lt;span class=&#34;math inline&#34;&gt;&lt;em&gt;n&lt;/em&gt;&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Using the same simulated time series for exogenous states in every iteration (as in other simulation algorithms in the literature) will allow this algorithm to converge; subject to a particular exogenous path. To ensure that the solution is accurate for the underlying data generating process and not just the simulation you use, make sure that the length of the time series is large.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>